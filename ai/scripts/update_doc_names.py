"""
Script ƒë·ªÉ update l·∫°i doc_name cho c√°c chunks trong Vector DB
C√≥ 2 options:
1. Update tr·ª±c ti·∫øp t·ª´ document text trong DB (nhanh nh∆∞ng c√≥ th·ªÉ kh√¥ng ch√≠nh x√°c 100%)
2. Re-process l·∫°i t·ª´ file g·ªëc (ch√≠nh x√°c nh·∫•t, khuy·∫øn ngh·ªã)
"""
import sys
import logging
from pathlib import Path
from typing import Dict, Any, List
from collections import defaultdict
import re

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from services.legal.vector_service import LegalVectorService
from services.legal.parser import LegalDocumentParser

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def update_doc_names():
    """
    Update doc_name cho t·∫•t c·∫£ chunks trong Vector DB
    S·ª≠ d·ª•ng method extract_doc_name m·ªõi
    """
    vector_service = LegalVectorService()
    parser = LegalDocumentParser()
    
    logger.info("=" * 80)
    logger.info("B·∫ÆT ƒê·∫¶U UPDATE DOC_NAMES TRONG VECTOR DB")
    logger.info("=" * 80)
    
    # Get t·∫•t c·∫£ documents t·ª´ collection
    logger.info("ƒêang l·∫•y t·∫•t c·∫£ chunks t·ª´ Vector DB...")
    all_results = vector_service._collection.get()
    
    if not all_results["ids"] or len(all_results["ids"]) == 0:
        logger.warning("Kh√¥ng c√≥ d·ªØ li·ªáu trong collection!")
        return
    
    total_chunks = len(all_results["ids"])
    logger.info(f"T√¨m th·∫•y {total_chunks} chunks")
    
    # Nh√≥m chunks theo doc_name hi·ªán t·∫°i
    chunks_by_doc_name = defaultdict(list)
    for i, metadata in enumerate(all_results["metadatas"]):
        old_doc_name = metadata.get("doc_name", "Unknown")
        chunks_by_doc_name[old_doc_name].append({
            "id": all_results["ids"][i],
            "document": all_results["documents"][i],
            "metadata": metadata,
            "embedding": all_results["embeddings"][i] if all_results.get("embeddings") else None
        })
    
    logger.info(f"T√¨m th·∫•y {len(chunks_by_doc_name)} vƒÉn b·∫£n kh√°c nhau")
    
    # ƒê·ªÉ update metadata, c·∫ßn l·∫•y embeddings t·ª´ collection
    # ChromaDB kh√¥ng l∆∞u embeddings khi get, c·∫ßn query l·∫°i ho·∫∑c get v·ªõi include=['embeddings']
    logger.info("ƒêang l·∫•y embeddings...")
    all_results_with_embeddings = vector_service._collection.get(include=['embeddings'])
    
    # T·∫°o mapping id -> embedding
    id_to_embedding = {}
    if all_results_with_embeddings.get("embeddings"):
        for i, chunk_id in enumerate(all_results_with_embeddings["ids"]):
            id_to_embedding[chunk_id] = all_results_with_embeddings["embeddings"][i]
    
    updated_count = 0
    unchanged_count = 0
    error_count = 0
    
    # Process t·ª´ng nh√≥m doc_name
    for old_doc_name, chunks in chunks_by_doc_name.items():
        logger.info(f"\nƒêang x·ª≠ l√Ω: '{old_doc_name}' ({len(chunks)} chunks)")
        
        # L·∫•y sample document ƒë·ªÉ extract doc_name m·ªõi
        # T√¨m chunk c√≥ document text ƒë·∫ßy ƒë·ªß nh·∫•t (kh√¥ng ph·∫£i text_for_embedding ƒë√£ enrich)
        sample_chunk = None
        for chunk in chunks:
            doc_text = chunk["document"]
            # T√¨m chunk c√≥ v·∫ª l√† original text (c√≥ ch·ª©a "ƒêi·ªÅu" ho·∫∑c c√°c t·ª´ kh√≥a ph√°p l√Ω)
            if doc_text and ("ƒêi·ªÅu" in doc_text or "Lu·∫≠t" in doc_text[:100]):
                sample_chunk = chunk
                break
        
        if not sample_chunk:
            sample_chunk = chunks[0]
        
        # Extract doc_name m·ªõi t·ª´ document text
        # Document text trong DB l√† text_for_embedding, c√≥ format:
        # "Lu·∫≠t Doanh nghi·ªáp 2020. Ch∆∞∆°ng 1: ... ƒêi·ªÅu 13: ..."
        # Ho·∫∑c c√≥ th·ªÉ l√†: "Lu·∫≠t. Doanh nghi·ªáp. 2020. Ch∆∞∆°ng..."
        doc_text = sample_chunk["document"]
        
        # Extract t·ª´ ƒë·∫ßu text_for_embedding
        # Format th∆∞·ªùng l√†: "DocName. Lo·∫°i: ... Ch∆∞∆°ng ... ƒêi·ªÅu ..."
        # T√¨m ph·∫ßn ƒë·∫ßu tr∆∞·ªõc "Lo·∫°i:", "Ch∆∞∆°ng", ho·∫∑c "ƒêi·ªÅu"
        new_doc_name = None
        
        # Pattern 1: T√¨m t·ª´ ƒë·∫ßu ƒë·∫øn d·∫•u ch·∫•m ƒë·∫ßu ti√™n ho·∫∑c "Lo·∫°i:"
        match = re.match(r'^([^.]*?)(?:\s*\.\s*Lo·∫°i:|\s*\.\s*Ch∆∞∆°ng|\s*\.\s*ƒêi·ªÅu|$)', doc_text, re.IGNORECASE)
        if match:
            potential_name = match.group(1).strip()
            # N·∫øu c√≥ ch·ª©a "Lu·∫≠t" v√† c√≥ ƒë·ªô d√†i h·ª£p l√Ω (> 5 k√Ω t·ª±)
            if 'lu·∫≠t' in potential_name.lower() and len(potential_name) > 5:
                # Chu·∫©n h√≥a: split by "." v√† join l·∫°i
                parts = [p.strip() for p in potential_name.split('.') if p.strip()]
                if parts:
                    # L·ªçc ra c√°c ph·∫ßn c√≥ v·∫ª l√† t√™n lu·∫≠t (kh√¥ng ph·∫£i ch·ªâ s·ªë ho·∫∑c t·ª´ ng·∫Øn)
                    name_parts = []
                    for part in parts:
                        if len(part) > 2 and not part.isdigit():
                            name_parts.append(part)
                        elif part.isdigit() and len(part) == 4:  # NƒÉm
                            name_parts.append(part)
                            break  # D·ª´ng sau nƒÉm
                    if name_parts:
                        new_doc_name = ' '.join(w.capitalize() for w in ' '.join(name_parts).split())
        
        # Pattern 2: N·∫øu kh√¥ng t√¨m ƒë∆∞·ª£c, th·ª≠ extract_doc_name (cho original text)
        if not new_doc_name or len(new_doc_name) < 5:
            # Th·ª≠ v·ªõi text nh∆∞ th·ªÉ l√† original (thay "." th√†nh "\n")
            text_for_extract = doc_text.replace('. ', '\n').replace('.', '\n')
            new_doc_name = parser.extract_doc_name(text_for_extract)
        
        # N·∫øu v·∫´n kh√¥ng c√≥, gi·ªØ nguy√™n
        if not new_doc_name or len(new_doc_name) < 5:
            logger.warning(f"  ‚ö†Ô∏è  Kh√¥ng th·ªÉ extract doc_name m·ªõi, gi·ªØ nguy√™n: '{old_doc_name}'")
            unchanged_count += len(chunks)
            continue
        
        if new_doc_name == old_doc_name:
            logger.info(f"  ‚úì Doc_name kh√¥ng thay ƒë·ªïi: '{new_doc_name}'")
            unchanged_count += len(chunks)
            continue
        
        logger.info(f"  üìù C·∫≠p nh·∫≠t: '{old_doc_name}' -> '{new_doc_name}'")
        
        # Update metadata cho t·∫•t c·∫£ chunks c·ªßa vƒÉn b·∫£n n√†y
        try:
            # Prepare data for upsert
            ids_to_update = []
            documents_to_update = []
            metadatas_to_update = []
            embeddings_to_update = []
            
            for chunk in chunks:
                chunk_id = chunk["id"]
                chunk_metadata = chunk["metadata"].copy()
                chunk_metadata["doc_name"] = new_doc_name
                
                ids_to_update.append(chunk_id)
                documents_to_update.append(chunk["document"])
                metadatas_to_update.append(chunk_metadata)
                
                # Get embedding n·∫øu c√≥
                if chunk_id in id_to_embedding:
                    embeddings_to_update.append(id_to_embedding[chunk_id])
                else:
                    logger.warning(f"  ‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y embedding cho chunk {chunk_id}")
                    # Kh√¥ng th·ªÉ update n·∫øu kh√¥ng c√≥ embedding
                    error_count += 1
                    continue
            
            # Upsert l·∫°i v·ªõi metadata m·ªõi (batch size 100)
            batch_size = 100
            for i in range(0, len(ids_to_update), batch_size):
                batch_ids = ids_to_update[i:i+batch_size]
                batch_docs = documents_to_update[i:i+batch_size]
                batch_metas = metadatas_to_update[i:i+batch_size]
                batch_embeddings = embeddings_to_update[i:i+batch_size]
                
                if len(batch_embeddings) == len(batch_ids):
                    vector_service._collection.upsert(
                        ids=batch_ids,
                        documents=batch_docs,
                        metadatas=batch_metas,
                        embeddings=batch_embeddings
                    )
                else:
                    logger.error(f"  ‚ùå S·ªë l∆∞·ª£ng embeddings kh√¥ng kh·ªõp v·ªõi s·ªë chunks")
                    error_count += len(batch_ids)
                    continue
            
            updated_count += len(ids_to_update)
            logger.info(f"  ‚úÖ ƒê√£ update {len(ids_to_update)} chunks")
            
        except Exception as e:
            logger.error(f"  ‚ùå L·ªói khi update: {e}", exc_info=True)
            error_count += len(chunks)
    
    logger.info("\n" + "=" * 80)
    logger.info("K·∫æT QU·∫¢ UPDATE:")
    logger.info(f"  ‚úÖ ƒê√£ update: {updated_count} chunks")
    logger.info(f"  ‚ûñ Kh√¥ng thay ƒë·ªïi: {unchanged_count} chunks")
    logger.info(f"  ‚ùå L·ªói: {error_count} chunks")
    logger.info("=" * 80)


if __name__ == "__main__":
    update_doc_names()
