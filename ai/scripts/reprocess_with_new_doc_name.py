"""
Script ƒë·ªÉ re-process l·∫°i documents v·ªõi method extract_doc_name m·ªõi
Script n√†y s·∫Ω:
1. Backup collection hi·ªán t·∫°i (optional)
2. X√≥a chunks c≈©
3. Re-process l·∫°i t·ª´ file g·ªëc v·ªõi method extract_doc_name m·ªõi
"""
import sys
import logging
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from services.legal.vector_service import LegalVectorService
from services.legal.parser import LegalDocumentParser
from services.legal.chunker import LegalDocumentChunker

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def reprocess_documents():
    """
    Re-process l·∫°i t·∫•t c·∫£ documents t·ª´ file g·ªëc v·ªõi method extract_doc_name m·ªõi
    """
    script_dir = Path(__file__).parent
    ai_dir = script_dir.parent
    input_dir = ai_dir / "luat_VN"
    
    if not input_dir.exists():
        logger.error(f"Input directory not found: {input_dir}")
        return
    
    vector_service = LegalVectorService()
    parser = LegalDocumentParser()
    chunker = LegalDocumentChunker()
    
    logger.info("=" * 80)
    logger.info("RE-PROCESS DOCUMENTS V·ªöI METHOD EXTRACT_DOC_NAME M·ªöI")
    logger.info("=" * 80)
    
    # Get stats hi·ªán t·∫°i
    old_stats = vector_service.get_collection_stats()
    logger.info(f"Chunks hi·ªán t·∫°i trong DB: {old_stats['total_chunks']}")
    
    # Confirm
    logger.warning("‚ö†Ô∏è  Script n√†y s·∫Ω X√ìA T·∫§T C·∫¢ chunks hi·ªán c√≥ v√† re-process l·∫°i!")
    logger.warning("‚ö†Ô∏è  ƒê·∫£m b·∫£o b·∫°n ƒë√£ backup n·∫øu c·∫ßn!")
    
    # Get all files
    pdf_files = list(input_dir.glob("*.pdf"))
    doc_files = list(input_dir.glob("*.doc")) + list(input_dir.glob("*.docx"))
    all_files = pdf_files + doc_files
    
    if not all_files:
        logger.warning(f"No PDF or DOC files found in {input_dir}")
        return
    
    logger.info(f"T√¨m th·∫•y {len(all_files)} files ƒë·ªÉ process")
    
    # X√≥a t·∫•t c·∫£ chunks c≈© (ho·∫∑c c√≥ th·ªÉ x√≥a t·ª´ng doc_name m·ªôt)
    logger.info("\nƒêang x√≥a chunks c≈©...")
    try:
        # Get all doc_names
        all_results = vector_service._collection.get()
        if all_results["ids"]:
            # X√≥a t·∫•t c·∫£
            vector_service._collection.delete(ids=all_results["ids"])
            logger.info(f"ƒê√£ x√≥a {len(all_results['ids'])} chunks c≈©")
    except Exception as e:
        logger.error(f"L·ªói khi x√≥a chunks c≈©: {e}")
        return
    
    # Process l·∫°i t·ª´ng file
    all_chunks = []
    
    for file_path in all_files:
        try:
            logger.info(f"\nProcessing: {file_path.name}")
            
            # Parse file
            text = parser.parse_file(file_path)
            logger.info(f"  ‚úì Parsed {len(text)} characters")
            
            # Extract metadata from filename
            file_metadata = parser.extract_metadata_from_filename(file_path)
            
            # Chunk document (s·∫Ω t·ª± ƒë·ªông d√πng extract_doc_name m·ªõi)
            chunks = chunker.chunk_document(
                text,
                filename=file_path.name,
                max_article_length=2000,
                max_clause_length=1000
            )
            
            logger.info(f"  ‚úì Created {len(chunks)} chunks")
            
            # Show sample doc_name
            if chunks:
                sample_doc_name = chunks[0]["metadata"].get("doc_name", "")
                logger.info(f"  üìù Doc name: '{sample_doc_name}'")
            
            # Update metadata with file info
            for chunk in chunks:
                if "source_id" not in chunk["metadata"] or not chunk["metadata"]["source_id"]:
                    chunk["metadata"]["source_id"] = file_metadata.get("source_id", "")
            
            all_chunks.extend(chunks)
            
        except ValueError as e:
            error_msg = str(e)
            if "PDF scan" in error_msg:
                logger.warning(f"  ‚ö†Ô∏è  Skipping {file_path.name}: {error_msg}")
            else:
                logger.warning(f"  ‚ö†Ô∏è  Skipping {file_path.name}: {error_msg}")
            continue
        except Exception as e:
            logger.error(f"  ‚ùå Error processing {file_path.name}: {e}", exc_info=True)
            continue
    
    if not all_chunks:
        logger.warning("No chunks created")
        return
    
    logger.info(f"\nT·ªïng s·ªë chunks: {len(all_chunks)}")
    
    # Embed chunks
    logger.info("Embedding chunks...")
    embedded_chunks = vector_service.embed_chunks(all_chunks, batch_size=20)
    
    # Upsert to ChromaDB
    logger.info("Upserting to ChromaDB...")
    vector_service.upsert_chunks(embedded_chunks, batch_size=100)
    
    # Print stats
    new_stats = vector_service.get_collection_stats()
    logger.info("\n" + "=" * 80)
    logger.info("‚úÖ RE-PROCESS HO√ÄN TH√ÄNH!")
    logger.info(f"  Chunks c≈©: {old_stats['total_chunks']}")
    logger.info(f"  Chunks m·ªõi: {new_stats['total_chunks']}")
    logger.info("=" * 80)


if __name__ == "__main__":
    reprocess_documents()
