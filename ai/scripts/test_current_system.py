"""
Script ƒë·ªÉ test v√† ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng h·ªá th·ªëng RAG hi·ªán t·∫°i
"""
import sys
import asyncio
import logging
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from services.legal.vector_service import LegalVectorService
from services.legal.legal_service import LegalAssistant

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def analyze_vectordb():
    """Ph√¢n t√≠ch VectorDB hi·ªán t·∫°i"""
    logger.info("="*80)
    logger.info("PH√ÇN T√çCH VECTORDB HI·ªÜN T·∫†I")
    logger.info("="*80)
    
    vector_service = LegalVectorService()
    
    # 1. Th·ªëng k√™ t·ªïng quan
    stats = vector_service.get_collection_stats()
    logger.info(f"\nüìä Th·ªëng k√™ t·ªïng quan:")
    logger.info(f"  - T·ªïng s·ªë chunks: {stats['total_chunks']}")
    logger.info(f"  - Collection: {stats['collection_name']}")
    
    # 2. L·∫•y sample chunks ƒë·ªÉ ph√¢n t√≠ch
    logger.info(f"\nüîç Ph√¢n t√≠ch sample chunks...")
    
    # Get all chunks (limited to first 100 for analysis)
    all_results = vector_service._collection.get(limit=100)
    
    if all_results["ids"]:
        logger.info(f"\nüìù Sample {min(10, len(all_results['ids']))} chunks ƒë·∫ßu ti√™n:")
        
        doc_names = {}
        chunk_sizes = []
        articles = {}
        
        for i in range(min(10, len(all_results["ids"]))):
            chunk_id = all_results["ids"][i]
            metadata = all_results["metadatas"][i]
            document = all_results["documents"][i]
            
            # Collect stats
            doc_name = metadata.get("doc_name", "Unknown")
            article = metadata.get("article", "")
            clause = metadata.get("clause", "")
            
            doc_names[doc_name] = doc_names.get(doc_name, 0) + 1
            chunk_sizes.append(len(document))
            
            if article:
                articles[article] = articles.get(article, 0) + 1
            
            logger.info(f"\n  Chunk {i+1}:")
            logger.info(f"    ID: {chunk_id}")
            logger.info(f"    Doc: {doc_name}")
            logger.info(f"    Article: {article}")
            logger.info(f"    Clause: {clause}")
            logger.info(f"    Text length: {len(document)} chars")
            logger.info(f"    Text preview: {document[:200]}...")
        
        # Statistics
        logger.info(f"\nüìà Th·ªëng k√™ chi ti·∫øt:")
        logger.info(f"  - S·ªë vƒÉn b·∫£n kh√°c nhau: {len(doc_names)}")
        logger.info(f"  - VƒÉn b·∫£n: {list(doc_names.keys())[:5]}")
        logger.info(f"  - K√≠ch th∆∞·ªõc chunk trung b√¨nh: {sum(chunk_sizes)/len(chunk_sizes):.0f} chars")
        logger.info(f"  - K√≠ch th∆∞·ªõc chunk min/max: {min(chunk_sizes)}/{max(chunk_sizes)} chars")
    
    return vector_service


async def test_search_quality(vector_service):
    """Test ch·∫•t l∆∞·ª£ng search v·ªõi c√°c c√¢u h·ªèi m·∫´u"""
    logger.info("\n" + "="*80)
    logger.info("TEST CH·∫§T L∆Ø·ª¢NG SEARCH")
    logger.info("="*80)
    
    # C√°c c√¢u h·ªèi test case
    test_queries = [
        {
            "query": "ƒêi·ªÅu ki·ªán th√†nh l·∫≠p doanh nghi·ªáp l√† g√¨?",
            "expected_doc": "Lu·∫≠t Doanh nghi·ªáp",
            "expected_article": "ƒêi·ªÅu 13"
        },
        {
            "query": "Ng∆∞·ªùi ƒë·∫°i di·ªán theo ph√°p lu·∫≠t c√≥ quy·ªÅn g√¨?",
            "expected_doc": "Lu·∫≠t Doanh nghi·ªáp",
            "expected_article": "ƒêi·ªÅu 13"
        },
        {
            "query": "Quy ƒë·ªãnh v·ªÅ thu·∫ø thu nh·∫≠p doanh nghi·ªáp",
            "expected_doc": "Lu·∫≠t Thu·∫ø",
            "expected_article": None
        },
        {
            "query": "Nghƒ©a v·ª• c·ªßa ng∆∞·ªùi ƒë·∫°i di·ªán ph√°p lu·∫≠t",
            "expected_doc": "Lu·∫≠t Doanh nghi·ªáp",
            "expected_article": "ƒêi·ªÅu 13"
        },
        {
            "query": "Th·ªß t·ª•c ƒëƒÉng k√Ω kinh doanh",
            "expected_doc": "Lu·∫≠t Doanh nghi·ªáp",
            "expected_article": None
        }
    ]
    
    results_summary = []
    
    for i, test_case in enumerate(test_queries, 1):
        query = test_case["query"]
        logger.info(f"\n{'='*60}")
        logger.info(f"Test Case {i}: {query}")
        logger.info(f"{'='*60}")
        
        # Search v·ªõi top_k=10
        search_results = vector_service.search(
            query=query,
            top_k=10,
            status="active"
        )
        
        if not search_results:
            logger.warning(f"  ‚ùå Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ n√†o!")
            results_summary.append({
                "query": query,
                "found": False,
                "top_doc": None,
                "top_article": None
            })
            continue
        
        # Ph√¢n t√≠ch k·∫øt qu·∫£
        logger.info(f"\n  üìä T√¨m th·∫•y {len(search_results)} k·∫øt qu·∫£:")
        
        for j, result in enumerate(search_results[:5], 1):
            metadata = result.get("metadata", {})
            doc_name = metadata.get("doc_name", "Unknown")
            article = metadata.get("article", "")
            clause = metadata.get("clause", "")
            point = metadata.get("point", "")
            distance = result.get("distance", 0)
            text = result.get("text", "")
            
            # Build reference
            ref_parts = []
            if article:
                ref_parts.append(article)
            if clause:
                ref_parts.append(f"Kho·∫£n {clause}")
            if point:
                ref_parts.append(f"ƒêi·ªÉm {point}")
            reference = ", ".join(ref_parts) if ref_parts else "N/A"
            
            logger.info(f"\n    K·∫øt qu·∫£ {j}:")
            logger.info(f"      VƒÉn b·∫£n: {doc_name}")
            logger.info(f"      Tham chi·∫øu: {reference}")
            logger.info(f"      Distance: {distance:.4f}")
            logger.info(f"      Text preview: {text[:150]}...")
        
        # Check if expected results are in top results
        top_result = search_results[0]
        top_metadata = top_result.get("metadata", {})
        top_doc = top_metadata.get("doc_name", "")
        top_article = top_metadata.get("article", "")
        
        expected_doc = test_case.get("expected_doc", "")
        expected_article = test_case.get("expected_article", "")
        
        # Evaluation
        doc_match = expected_doc.lower() in top_doc.lower() if expected_doc else True
        article_match = expected_article in top_article if expected_article else True
        
        if doc_match and article_match:
            logger.info(f"\n  ‚úÖ PASS: K·∫øt qu·∫£ ƒë√∫ng!")
        elif doc_match:
            logger.info(f"\n  ‚ö†Ô∏è  PARTIAL: ƒê√∫ng vƒÉn b·∫£n nh∆∞ng sai ƒêi·ªÅu")
            logger.info(f"      Expected: {expected_article}, Got: {top_article}")
        else:
            logger.info(f"\n  ‚ùå FAIL: Sai vƒÉn b·∫£n")
            logger.info(f"      Expected: {expected_doc}, Got: {top_doc}")
        
        results_summary.append({
            "query": query,
            "found": True,
            "top_doc": top_doc,
            "top_article": top_article,
            "expected_doc": expected_doc,
            "expected_article": expected_article,
            "doc_match": doc_match,
            "article_match": article_match
        })
    
    # Summary
    logger.info("\n" + "="*80)
    logger.info("üìä T·ªîNG K·∫æT TEST")
    logger.info("="*80)
    
    total = len(results_summary)
    found = sum(1 for r in results_summary if r["found"])
    doc_correct = sum(1 for r in results_summary if r.get("doc_match", False))
    article_correct = sum(1 for r in results_summary if r.get("article_match", False))
    fully_correct = sum(1 for r in results_summary if r.get("doc_match", False) and r.get("article_match", False))
    
    logger.info(f"\nK·∫øt qu·∫£:")
    logger.info(f"  - T·ªïng s·ªë test: {total}")
    logger.info(f"  - T√¨m th·∫•y k·∫øt qu·∫£: {found}/{total} ({found/total*100:.1f}%)")
    logger.info(f"  - ƒê√∫ng vƒÉn b·∫£n: {doc_correct}/{total} ({doc_correct/total*100:.1f}%)")
    logger.info(f"  - ƒê√∫ng ƒëi·ªÅu: {article_correct}/{total} ({article_correct/total*100:.1f}%)")
    logger.info(f"  - Ho√†n to√†n ch√≠nh x√°c: {fully_correct}/{total} ({fully_correct/total*100:.1f}%)")
    
    return results_summary


async def test_end_to_end():
    """Test end-to-end v·ªõi Legal Assistant"""
    logger.info("\n" + "="*80)
    logger.info("TEST END-TO-END V·ªöI LEGAL ASSISTANT")
    logger.info("="*80)
    
    assistant = LegalAssistant()
    
    test_queries = [
        "Ng∆∞·ªùi ƒë·∫°i di·ªán theo ph√°p lu·∫≠t c√≥ nh·ªØng quy·ªÅn g√¨?",
        "ƒêi·ªÅu ki·ªán ƒë·ªÉ th√†nh l·∫≠p c√¥ng ty l√† g√¨?",
        "L∆∞∆°ng 50 tri·ªáu ƒë√≥ng thu·∫ø bao nhi√™u?"
    ]
    
    for i, query in enumerate(test_queries, 1):
        logger.info(f"\n{'='*60}")
        logger.info(f"Query {i}: {query}")
        logger.info(f"{'='*60}")
        
        try:
            response = await assistant.process_query(query, region=1)
            logger.info(f"\nüìù Response:")
            logger.info(f"{response[:500]}...")
            logger.info(f"\n(Total length: {len(response)} chars)")
        except Exception as e:
            logger.error(f"‚ùå Error: {e}", exc_info=True)


async def main():
    """Main test function"""
    logger.info("\n" + "="*80)
    logger.info("üß™ B·∫ÆT ƒê·∫¶U KI·ªÇM TRA H·ªÜ TH·ªêNG")
    logger.info("="*80)
    
    # Step 1: Analyze VectorDB
    vector_service = analyze_vectordb()
    
    # Step 2: Test search quality
    await test_search_quality(vector_service)
    
    # Step 3: Test end-to-end
    await test_end_to_end()
    
    logger.info("\n" + "="*80)
    logger.info("‚úÖ HO√ÄN TH√ÄNH KI·ªÇM TRA")
    logger.info("="*80)
    
    # Recommendations
    logger.info("\n" + "="*80)
    logger.info("üí° KHUY·∫æN NGH·ªä")
    logger.info("="*80)
    logger.info("""
D·ª±a tr√™n k·∫øt qu·∫£ test, c√°c khuy·∫øn ngh·ªã:

1. N·∫øu accuracy < 70%:
   ‚Üí C·∫ßn implement OPTION A (Context Injection Enhancement)
   ‚Üí Th·ªùi gian: ~30 ph√∫t
   ‚Üí R·ªßi ro: Th·∫•p

2. N·∫øu accuracy 70-85%:
   ‚Üí C√≥ th·ªÉ implement OPTION C (Hybrid: Context + Re-ranking)
   ‚Üí Th·ªùi gian: ~1 gi·ªù
   ‚Üí R·ªßi ro: Trung b√¨nh

3. N·∫øu accuracy > 85%:
   ‚Üí H·ªá th·ªëng ƒë√£ t·ªët, ch·ªâ c·∫ßn fine-tune
   ‚Üí TƒÉng top_k ho·∫∑c ƒëi·ªÅu ch·ªânh prompt
   ‚Üí Th·ªùi gian: ~15 ph√∫t
   ‚Üí R·ªßi ro: R·∫•t th·∫•p

4. N·∫øu c√≥ v·∫•n ƒë·ªÅ v·ªÅ d·∫´n ch·ª©ng sai ƒêi·ªÅu/Kho·∫£n:
   ‚Üí Implement OPTION B (Full Overlap Chunking)
   ‚Üí Th·ªùi gian: ~2-3 gi·ªù
   ‚Üí R·ªßi ro: Cao (c·∫ßn re-chunk to√†n b·ªô)
""")


if __name__ == "__main__":
    asyncio.run(main())
