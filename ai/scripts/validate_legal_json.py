"""
Script ƒë·ªÉ validate ch·∫•t l∆∞·ª£ng JSON legal documents
Ki·ªÉm tra c√°c v·∫•n ƒë·ªÅ c√≥ th·ªÉ l√†m gi·∫£m ƒë·ªô ch√≠nh x√°c c·ªßa Vector Search v√† RAG Chatbot
"""
import sys
import json
import logging
from pathlib import Path
from typing import Dict, Any, List

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def validate_json(file_path: Path) -> Dict[str, Any]:
    """
    Validate ch·∫•t l∆∞·ª£ng JSON legal documents
    
    Checks:
    1. doc_name kh√¥ng ƒë∆∞·ª£c l√† filename ho·∫∑c ch·ª©a ƒëu√¥i file
    2. text_for_embedding ph·∫£i ch·ª©a t√™n lu·∫≠t chu·∫©n (kh√¥ng ph·∫£i filename)
    3. Metadata ph·∫£i ƒë·∫ßy ƒë·ªß
    4. text_for_embedding ph·∫£i c√≥ context injection
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    documents = data.get('documents', [])
    summary = data.get('summary', {})
    
    logger.info("="*80)
    logger.info("VALIDATE LEGAL DOCUMENTS JSON")
    logger.info("="*80)
    logger.info(f"Total documents: {summary.get('total_documents', 0)}")
    logger.info(f"Total chunks: {summary.get('total_chunks', 0)}")
    logger.info("="*80 + "\n")
    
    issues = {
        "critical": [],  # L·ªói ch√≠ m·∫°ng - ph·∫£i s·ª≠a ngay
        "warning": [],   # C·∫£nh b√°o - n√™n s·ª≠a
        "info": []       # Th√¥ng tin - c√≥ th·ªÉ c·∫£i thi·ªán
    }
    
    for doc_idx, doc in enumerate(documents):
        info = doc.get('document_info', {})
        filename = info.get('filename', 'Unknown')
        doc_name = info.get('doc_name', '')
        chunks = doc.get('chunks', [])
        
        logger.info(f"\nüìÑ Document {doc_idx + 1}: {filename}")
        logger.info(f"   Doc name: {doc_name}")
        logger.info(f"   Chunks: {len(chunks)}")
        
        # === CRITICAL CHECKS ===
        
        # Check 1: doc_name kh√¥ng ƒë∆∞·ª£c l√† filename ho·∫∑c ch·ª©a ƒëu√¥i file
        if not doc_name:
            issues["critical"].append({
                "file": filename,
                "issue": "doc_name r·ªóng",
                "message": "doc_name kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng - s·∫Ω l√†m nhi·ªÖu vector search"
            })
            logger.error(f"   ‚ùå CRITICAL: doc_name r·ªóng!")
        elif doc_name == filename:
            issues["critical"].append({
                "file": filename,
                "issue": "doc_name gi·ªëng filename",
                "message": f"doc_name='{doc_name}' gi·ªëng filename - c·∫ßn mapping t√™n lu·∫≠t chu·∫©n"
            })
            logger.error(f"   ‚ùå CRITICAL: doc_name gi·ªëng filename: '{doc_name}'")
        elif any(ext in doc_name.lower() for ext in ['.pdf', '.docx', '.doc']):
            issues["critical"].append({
                "file": filename,
                "issue": "doc_name ch·ª©a ƒëu√¥i file",
                "message": f"doc_name='{doc_name}' ch·ª©a ƒëu√¥i file - c·∫ßn l√†m s·∫°ch"
            })
            logger.error(f"   ‚ùå CRITICAL: doc_name ch·ª©a ƒëu√¥i file: '{doc_name}'")
        elif any(pattern in doc_name.lower() for pattern in ['vbhn', 'vpqh', '-vbhn-', '_vbhn_']):
            issues["warning"].append({
                "file": filename,
                "issue": "doc_name ch·ª©a pattern filename",
                "message": f"doc_name='{doc_name}' v·∫´n c√≤n pattern filename - n√™n l√†m s·∫°ch h∆°n"
            })
            logger.warning(f"   ‚ö†Ô∏è  WARNING: doc_name c√≤n pattern filename: '{doc_name}'")
        else:
            logger.info(f"   ‚úÖ doc_name OK: '{doc_name}'")
        
        # Check 2: text_for_embedding ph·∫£i ch·ª©a t√™n lu·∫≠t chu·∫©n
        chunks_with_issues = 0
        for chunk_idx, chunk in enumerate(chunks):
            text_for_embedding = chunk.get('text_for_embedding', '')
            chunk_id = chunk.get('id', f'chunk_{chunk_idx}')
            
            if not text_for_embedding:
                issues["critical"].append({
                    "file": filename,
                    "chunk_id": chunk_id,
                    "issue": "text_for_embedding r·ªóng",
                    "message": "text_for_embedding kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng"
                })
                chunks_with_issues += 1
                continue
            
            # Check n·∫øu text_for_embedding ch·ª©a filename thay v√¨ t√™n lu·∫≠t
            if filename.lower().replace('.pdf', '').replace('.docx', '').replace('.doc', '') in text_for_embedding.lower():
                if doc_name and doc_name.lower() not in text_for_embedding.lower():
                    # text_for_embedding ch·ª©a filename nh∆∞ng kh√¥ng ch·ª©a doc_name chu·∫©n
                    issues["critical"].append({
                        "file": filename,
                        "chunk_id": chunk_id,
                        "issue": "text_for_embedding ch·ª©a filename",
                        "message": f"text_for_embedding c√≥ th·ªÉ ch·ª©a filename pattern - c·∫ßn ki·ªÉm tra",
                        "preview": text_for_embedding[:100] + "..."
                    })
                    chunks_with_issues += 1
            
            # Check context injection
            if "Lu·∫≠t:" not in text_for_embedding and "lu·∫≠t:" not in text_for_embedding:
                issues["warning"].append({
                    "file": filename,
                    "chunk_id": chunk_id,
                    "issue": "text_for_embedding thi·∫øu context 'Lu·∫≠t:'",
                    "message": "text_for_embedding n√™n c√≥ format 'Lu·∫≠t: {doc_name}...' ƒë·ªÉ context injection t·ªët h∆°n",
                    "preview": text_for_embedding[:100] + "..."
                })
            
            # Check n·∫øu text_for_embedding qu√° ng·∫Øn (c√≥ th·ªÉ thi·∫øu context)
            if len(text_for_embedding) < 50:
                issues["warning"].append({
                    "file": filename,
                    "chunk_id": chunk_id,
                    "issue": "text_for_embedding qu√° ng·∫Øn",
                    "message": f"text_for_embedding ch·ªâ c√≥ {len(text_for_embedding)} k√Ω t·ª± - c√≥ th·ªÉ thi·∫øu context"
                })
        
        if chunks_with_issues > 0:
            logger.error(f"   ‚ùå CRITICAL: {chunks_with_issues}/{len(chunks)} chunks c√≥ v·∫•n ƒë·ªÅ v·ªõi text_for_embedding")
        else:
            logger.info(f"   ‚úÖ T·∫•t c·∫£ chunks c√≥ text_for_embedding h·ª£p l·ªá")
        
        # Check 3: Metadata ƒë·∫ßy ƒë·ªß
        missing_metadata = []
        required_fields = ["doc_name", "doc_type"]
        for field in required_fields:
            if not info.get(field):
                missing_metadata.append(field)
        
        if missing_metadata:
            issues["warning"].append({
                "file": filename,
                "issue": "metadata thi·∫øu",
                "message": f"Metadata thi·∫øu c√°c tr∆∞·ªùng: {', '.join(missing_metadata)}"
            })
            logger.warning(f"   ‚ö†Ô∏è  WARNING: Metadata thi·∫øu: {', '.join(missing_metadata)}")
        else:
            logger.info(f"   ‚úÖ Metadata ƒë·∫ßy ƒë·ªß")
    
    # Print summary
    logger.info("\n" + "="*80)
    logger.info("VALIDATION SUMMARY")
    logger.info("="*80)
    
    total_critical = len(issues["critical"])
    total_warning = len(issues["warning"])
    total_info = len(issues["info"])
    
    logger.info(f"‚ùå Critical issues: {total_critical}")
    logger.info(f"‚ö†Ô∏è  Warnings: {total_warning}")
    logger.info(f"‚ÑπÔ∏è  Info: {total_info}")
    
    if total_critical > 0:
        logger.error("\nüö® CRITICAL ISSUES (Ph·∫£i s·ª≠a ngay):")
        for issue in issues["critical"][:10]:  # Hi·ªÉn th·ªã 10 ƒë·∫ßu ti√™n
            logger.error(f"  ‚Ä¢ {issue['file']}: {issue['issue']}")
            logger.error(f"    ‚Üí {issue['message']}")
            if 'preview' in issue:
                logger.error(f"    Preview: {issue['preview']}")
        if total_critical > 10:
            logger.error(f"  ... v√† {total_critical - 10} issues kh√°c")
    
    if total_warning > 0:
        logger.warning("\n‚ö†Ô∏è  WARNINGS (N√™n s·ª≠a):")
        for issue in issues["warning"][:5]:  # Hi·ªÉn th·ªã 5 ƒë·∫ßu ti√™n
            logger.warning(f"  ‚Ä¢ {issue['file']}: {issue['issue']}")
            if 'chunk_id' in issue:
                logger.warning(f"    Chunk: {issue['chunk_id']}")
    
    logger.info("="*80)
    
    # Return result
    return {
        "total_documents": len(documents),
        "total_chunks": sum(len(doc.get('chunks', [])) for doc in documents),
        "issues": issues,
        "status": "PASS" if total_critical == 0 else "FAIL"
    }


def main():
    """Main function"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Validate ch·∫•t l∆∞·ª£ng JSON legal documents"
    )
    parser.add_argument(
        "--input",
        type=str,
        default="legal_documents.json",
        help="File JSON c·∫ßn validate (m·∫∑c ƒë·ªãnh: legal_documents.json)"
    )
    
    args = parser.parse_args()
    
    # Get input file
    script_dir = Path(__file__).parent
    input_file = script_dir / args.input
    
    if not input_file.exists():
        logger.error(f"File not found: {input_file}")
        return
    
    # Validate
    result = validate_json(input_file)
    
    # Exit code
    if result["status"] == "FAIL":
        sys.exit(1)
    else:
        logger.info("\n‚úÖ Validation passed!")
        sys.exit(0)


if __name__ == "__main__":
    main()





