# ü§ñ AI Service - E-commerce Legal Assistant

H·ªá th·ªëng AI t∆∞ v·∫•n ph√°p lu·∫≠t cho doanh nghi·ªáp e-commerce, s·ª≠ d·ª•ng RAG (Retrieval-Augmented Generation) ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ lu·∫≠t doanh nghi·ªáp, thu·∫ø, lao ƒë·ªông.

---

## üìã M·ª•c L·ª•c

1. [T·ªïng Quan](#-t·ªïng-quan)
2. [C√¥ng Ngh·ªá](#-c√¥ng-ngh·ªá)
3. [C·∫•u Tr√∫c Project](#-c·∫•u-tr√∫c-project)
4. [C√†i ƒê·∫∑t](#-c√†i-ƒë·∫∑t)
5. [S·ª≠ D·ª•ng](#-s·ª≠-d·ª•ng)
6. [Workflow Chi Ti·∫øt](#-workflow-chi-ti·∫øt)
7. [API Endpoints](#-api-endpoints)
8. [Testing](#-testing)
9. [Troubleshooting](#-troubleshooting)

---

## üéØ T·ªïng Quan

### **Ch·ª©c NƒÉng Ch√≠nh:**

1. **Legal Assistant (Chatbot Admin):**
   - T∆∞ v·∫•n ph√°p lu·∫≠t doanh nghi·ªáp
   - Tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ thu·∫ø, lao ƒë·ªông, ƒë·∫ßu t∆∞
   - D·∫´n ch·ª©ng ch√≠nh x√°c t·ª´ vƒÉn b·∫£n ph√°p lu·∫≠t

2. **Tax Calculator:**
   - T√≠nh thu·∫ø thu nh·∫≠p c√° nh√¢n (TNCN)
   - T√≠nh b·∫£o hi·ªÉm (BHXH, BHYT, BHTN)
   - Hi·ªÉn th·ªã chi ti·∫øt c√°c kho·∫£n gi·∫£m tr·ª´

3. **User Chatbot:**
   - H·ªó tr·ª£ kh√°ch h√†ng
   - T√¨m ki·∫øm s·∫£n ph·∫©m
   - Tr·∫£ l·ªùi FAQ

### **K·∫øt Qu·∫£ ƒê·∫°t ƒê∆∞·ª£c:**

| Metric | K·∫øt Qu·∫£ | ƒê√°nh Gi√° |
|--------|---------|----------|
| **ƒê√∫ng vƒÉn b·∫£n** | 100% (5/5) | ‚úÖ Excellent |
| **ƒê√∫ng ƒêi·ªÅu** | 40% (2/5) | ‚ö†Ô∏è Acceptable |
| **Total chunks** | 1,621 | ‚úÖ Good coverage |
| **Documents** | 7 lu·∫≠t | ‚úÖ Complete |

---

## üõ†Ô∏è C√¥ng Ngh·ªá

### **Core Technologies:**

- **Backend:** Python 3.11, Flask
- **Vector Database:** ChromaDB
- **Embedding Model:** intfloat/multilingual-e5-small
- **LLM:** Google Gemini 1.5 Flash
- **PDF Parsing:** PyMuPDF, python-docx

### **Chunking Strategy:**

Document Structure-based Chunking:
```
VƒÉn b·∫£n ph√°p lu·∫≠t
  ‚îî‚îÄ‚îÄ Ch∆∞∆°ng (Chapter)
      ‚îî‚îÄ‚îÄ ƒêi·ªÅu (Article)
          ‚îî‚îÄ‚îÄ Kho·∫£n (Clause)
              ‚îî‚îÄ‚îÄ ƒêi·ªÉm (Point)
```

M·ªói chunk ch·ª©a:
- Text g·ªëc c·ªßa ƒêi·ªÅu/Kho·∫£n/ƒêi·ªÉm
- Context injection (t√™n lu·∫≠t, ch∆∞∆°ng, ƒëi·ªÅu)
- Metadata (doc_name, article, chapter, source_id)

---

## üìÅ C·∫•u Tr√∫c Project

```
ai/
‚îú‚îÄ‚îÄ README.md                       # File n√†y
‚îú‚îÄ‚îÄ app.py                          # Main Flask application
‚îú‚îÄ‚îÄ prompts.py                      # LLM prompts
‚îú‚îÄ‚îÄ agents.py                       # Agent definitions
‚îÇ
‚îú‚îÄ‚îÄ core/                           # Core modules
‚îÇ   ‚îú‚îÄ‚îÄ config.py                   # Configuration
‚îÇ   ‚îú‚îÄ‚îÄ conversation.py             # Conversation management
‚îÇ   ‚îú‚îÄ‚îÄ db.py                       # Database connection
‚îÇ   ‚îú‚îÄ‚îÄ exceptions.py               # Custom exceptions
‚îÇ   ‚îú‚îÄ‚îÄ logging.py                  # Logging setup
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                    # Utility functions
‚îÇ
‚îú‚îÄ‚îÄ shared/                         # Shared modules
‚îÇ   ‚îú‚îÄ‚îÄ llm_client.py               # LLM client (Gemini)
‚îÇ   ‚îî‚îÄ‚îÄ models.py                   # Data models
‚îÇ
‚îú‚îÄ‚îÄ services/                       # Business logic
‚îÇ   ‚îú‚îÄ‚îÄ legal/                      # Legal RAG system ‚≠ê
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ parser.py               # Parse PDF/DOC ‚Üí text
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chunker.py              # Chunk theo c·∫•u tr√∫c lu·∫≠t
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vector_service.py       # Embedding & ChromaDB
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ legal_service.py        # RAG pipeline (MAIN)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ improved_legal_service.py  # Enhanced version
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tax_calculator.py       # Tax calculation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ constants.py            # Constants
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ chatbot/                    # Chatbot services
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ search.py               # Search service
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ improved_user_chatbot.py  # User chatbot
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ analyst/                    # Analytics
‚îÇ   ‚îú‚îÄ‚îÄ report/                     # Reporting
‚îÇ   ‚îú‚îÄ‚îÄ sentiment/                  # Sentiment analysis
‚îÇ   ‚îî‚îÄ‚îÄ moderation/                 # Content moderation
‚îÇ
‚îú‚îÄ‚îÄ scripts/                        # Utility scripts ‚≠ê
‚îÇ   ‚îú‚îÄ‚îÄ parse_to_json.py            # Parse PDF ‚Üí JSON
‚îÇ   ‚îú‚îÄ‚îÄ embed_from_json.py          # Embed JSON ‚Üí VectorDB
‚îÇ   ‚îú‚îÄ‚îÄ test_current_system.py      # Test accuracy
‚îÇ   ‚îî‚îÄ‚îÄ reprocess_legal_documents.py  # Main reprocess script
‚îÇ
‚îú‚îÄ‚îÄ luat_VN/                        # Legal documents (PDF/DOC)
‚îÇ   ‚îú‚îÄ‚îÄ 67-VBHN-VPQH.docx          # Lu·∫≠t Doanh Nghi·ªáp 2020
‚îÇ   ‚îú‚îÄ‚îÄ 125-vbhn-vpqh.pdf          # B·ªô Lu·∫≠t Lao ƒê·ªông 2019
‚îÇ   ‚îú‚îÄ‚îÄ 134-vbhn-vpqh.pdf          # Lu·∫≠t ƒê·∫ßu T∆∞ 2020
‚îÇ   ‚îú‚îÄ‚îÄ thue_gtgt.pdf              # Lu·∫≠t Thu·∫ø GTGT 2024
‚îÇ   ‚îú‚îÄ‚îÄ 103-vbhn-vpqh.pdf          # Lu·∫≠t Thu·∫ø TNCN 2007
‚îÇ   ‚îú‚îÄ‚îÄ 2023_575+576_22-VBHN-VPQH.pdf  # Lu·∫≠t Thu·∫ø TNDN 2008
‚îÇ   ‚îî‚îÄ‚îÄ 123.signed_01.pdf          # Ngh·ªã ƒë·ªãnh 123 (scan)
‚îÇ
‚îú‚îÄ‚îÄ chroma_db/                      # VectorDB storage
‚îÇ   ‚îî‚îÄ‚îÄ legal_documents/            # Legal chunks collection
‚îÇ
‚îî‚îÄ‚îÄ venv/                           # Python virtual environment
```

---

## ‚öôÔ∏è C√†i ƒê·∫∑t

### **1. Prerequisites:**

```bash
# Python 3.11+
python --version

# Node.js (for backend server)
node --version
```

### **2. Setup Virtual Environment:**

```bash
cd /path/to/web-ecommerce/ai

# Create virtual environment
python3 -m venv venv

# Activate
source venv/bin/activate  # macOS/Linux
# ho·∫∑c
venv\Scripts\activate  # Windows
```

### **3. Install Dependencies:**

```bash
pip install -r requirements.txt
```

**Key dependencies:**
- `chromadb` - Vector database
- `sentence-transformers` - Embedding model
- `google-generativeai` - Gemini LLM
- `PyMuPDF` - PDF parsing
- `python-docx` - DOC parsing
- `flask` - Web framework

### **4. Environment Variables:**

T·∫°o file `.env` trong th∆∞ m·ª•c `ai/`:

```env
# Gemini API Key
GEMINI_API_KEY=your_gemini_api_key_here

# Database
DATABASE_URL=postgresql://user:password@localhost:5432/ecommerce

# ChromaDB
CHROMA_DB_PATH=./chroma_db

# Flask
FLASK_ENV=development
FLASK_DEBUG=True
```

---

## üöÄ S·ª≠ D·ª•ng

### **A. S·ª≠ D·ª•ng Chatbot (Qua UI)**

#### **1. Start Backend Server:**

```bash
# Terminal 1: Start AI service
cd /path/to/web-ecommerce/ai
source venv/bin/activate
python app.py

# Terminal 2: Start backend server
cd /path/to/web-ecommerce/backend
npm run dev
```

#### **2. Access Admin Chatbot:**

```
URL: http://localhost:5000/admin/chatbot
```

#### **3. Sample Queries:**

**Legal Questions:**
```
- "ƒêi·ªÅu ki·ªán th√†nh l·∫≠p c√¥ng ty TNHH l√† g√¨?"
- "Ng∆∞·ªùi ƒë·∫°i di·ªán theo ph√°p lu·∫≠t c√≥ nh·ªØng quy·ªÅn g√¨?"
- "Th·ªß t·ª•c ƒëƒÉng k√Ω kinh doanh nh∆∞ th·∫ø n√†o?"
- "Quy ƒë·ªãnh v·ªÅ thu·∫ø GTGT cho s·∫£n ph·∫©m ƒëi·ªán t·ª≠?"
```

**Tax Calculation:**
```
- "L∆∞∆°ng 50 tri·ªáu ƒë√≥ng thu·∫ø bao nhi√™u?"
- "T√≠nh thu·∫ø TNCN cho l∆∞∆°ng 30 tri·ªáu, 2 ng∆∞·ªùi ph·ª• thu·ªôc"
- "Gi·∫£m tr·ª´ gia c·∫£nh l√† bao nhi√™u?"
```

---

### **B. Re-process Legal Documents (Khi C·∫ßn)**

#### **Khi n√†o c·∫ßn re-process?**

- Th√™m vƒÉn b·∫£n ph√°p lu·∫≠t m·ªõi
- C·∫≠p nh·∫≠t vƒÉn b·∫£n hi·ªán c√≥
- VectorDB b·ªã corrupt
- Thay ƒë·ªïi chunking strategy

#### **Workflow:**

```bash
cd /path/to/web-ecommerce/ai
source venv/bin/activate

# B∆∞·ªõc 1: Parse PDF/DOC ‚Üí JSON
python scripts/parse_to_json.py

# B∆∞·ªõc 2: Review JSON
# M·ªü file scripts/legal_documents.json ƒë·ªÉ ki·ªÉm tra

# B∆∞·ªõc 3: Embed JSON ‚Üí VectorDB (x√≥a data c≈©)
python scripts/embed_from_json.py --clear
# Nh·∫≠p 'yes' khi ƒë∆∞·ª£c h·ªèi confirm

# B∆∞·ªõc 4: Test accuracy
python scripts/test_current_system.py
```

#### **Output:**

```
‚úÖ Embedding complete!
Total chunks in DB: 1621

üîç Verifying by sampling...
Query: 'Ng∆∞·ªùi ƒë·∫°i di·ªán theo ph√°p lu·∫≠t'
  Top result:
    Doc: Lu·∫≠t Doanh Nghi·ªáp 2020
    Article: ƒêi·ªÅu 12
    Distance: 0.2065
```

---

### **C. Th√™m VƒÉn B·∫£n Ph√°p Lu·∫≠t M·ªõi**

#### **B∆∞·ªõc 1: Th√™m file PDF/DOC**

```bash
# Copy file v√†o th∆∞ m·ª•c luat_VN/
cp /path/to/new_law.pdf ai/luat_VN/
```

#### **B∆∞·ªõc 2: Update mapping trong parser**

M·ªü file `services/legal/parser.py`, t√¨m method `guess_law_name_from_filename()`:

```python
def guess_law_name_from_filename(self, filename: str) -> str:
    mapping = {
        # ... existing mappings ...
        
        # Th√™m mapping m·ªõi
        "new_law": "Lu·∫≠t M·ªõi 2024",
        "123-new": "Lu·∫≠t M·ªõi 2024",
    }
    # ...
```

#### **B∆∞·ªõc 3: Re-process**

```bash
python scripts/parse_to_json.py
python scripts/embed_from_json.py --clear
python scripts/test_current_system.py
```

---

## üîÑ Workflow Chi Ti·∫øt

### **1. Parse PDF/DOC ‚Üí Text**

**File:** `services/legal/parser.py`

**Input:** PDF/DOC file

**Process:**
1. Detect file type (PDF/DOC)
2. Extract raw text
3. Clean text (remove noise, normalize whitespace)
4. Extract document name
5. Extract metadata from filename

**Output:** 
```python
{
    "text": "LU·∫¨T\nDOANH NGHI·ªÜP\n...",
    "doc_name": "Lu·∫≠t Doanh Nghi·ªáp 2020",
    "source_id": "67"
}
```

**Key Methods:**
- `parse_file()` - Main parsing
- `extract_doc_name()` - Extract t√™n lu·∫≠t
- `guess_law_name_from_filename()` - Fallback mapping

---

### **2. Chunk Document ‚Üí Structured Chunks**

**File:** `services/legal/chunker.py`

**Input:** Raw text + metadata

**Process:**
1. Split by Chapters (Ch∆∞∆°ng)
2. Split by Articles (ƒêi·ªÅu)
3. Split by Clauses (Kho·∫£n)
4. Split by Points (ƒêi·ªÉm)
5. Inject context for embedding
6. Generate unique chunk IDs

**Output:**
```python
{
    "id": "67_VBHN_VPQH_D13_K1",
    "text_for_embedding": "Lu·∫≠t: Lu·∫≠t Doanh Nghi·ªáp 2020. ƒêi·ªÅu 13: Tr√°ch nhi·ªám c·ªßa ng∆∞·ªùi ƒë·∫°i di·ªán theo ph√°p lu·∫≠t...",
    "metadata": {
        "doc_name": "Lu·∫≠t Doanh Nghi·ªáp 2020",
        "doc_type": "Lu·∫≠t",
        "article": "ƒêi·ªÅu 13",
        "article_title": "Tr√°ch nhi·ªám c·ªßa ng∆∞·ªùi ƒë·∫°i di·ªán theo ph√°p lu·∫≠t",
        "chapter": "Ch∆∞∆°ng I",
        "source_id": "67",
        "status": "active"
    }
}
```

**Key Methods:**
- `chunk_document()` - Main chunking logic
- `enrich_text_for_embedding()` - Add context
- `generate_chunk_id()` - Generate unique ID

---

### **3. Embed & Store ‚Üí VectorDB**

**File:** `services/legal/vector_service.py`

**Input:** List of chunks

**Process:**
1. Load embedding model (multilingual-e5-small)
2. Generate embeddings in batches
3. Handle OOM (Out of Memory) gracefully
4. Upsert to ChromaDB

**Output:** VectorDB v·ªõi 1,621 chunks

**Key Methods:**
- `embed_chunks()` - Generate embeddings
- `upsert_chunks()` - Store to ChromaDB
- `search()` - Vector similarity search

**Embedding Model:**
- Model: `intfloat/multilingual-e5-small`
- Dimension: 384
- Language: Multilingual (Vietnamese supported)

---

### **4. RAG Pipeline ‚Üí Answer**

**File:** `services/legal/legal_service.py`

**Input:** User query

**Process:**
1. **Intent Classification:**
   - Legal query ‚Üí RAG pipeline
   - Tax query ‚Üí Tax calculator

2. **Vector Search:**
   - Embed query
   - Search top_k=30 chunks
   - Filter by doc_type, status

3. **Context Construction:**
   - Group chunks by document
   - Format with citations
   - Add metadata

4. **LLM Generation:**
   - Send context + query to Gemini
   - Generate answer with citations
   - Format response

**Output:**
```
D·ª±a v√†o c√°c vƒÉn b·∫£n ph√°p lu·∫≠t sau ƒë√¢y:

---
[VƒÉn b·∫£n 1] Lu·∫≠t - Lu·∫≠t Doanh Nghi·ªáp 2020
Tham chi·∫øu: ƒêi·ªÅu 13, "Tr√°ch nhi·ªám c·ªßa ng∆∞·ªùi ƒë·∫°i di·ªán theo ph√°p lu·∫≠t"
N·ªôi dung: ...

Tr·∫£ l·ªùi: ...
```

**Key Methods:**
- `handle_query()` - Main entry point
- `_handle_legal_query()` - RAG pipeline
- `_handle_tax_query()` - Tax calculator

---

## üåê API Endpoints

### **1. Legal Consultation**

```http
POST /api/legal/consult
Content-Type: application/json

{
  "query": "ƒêi·ªÅu ki·ªán th√†nh l·∫≠p c√¥ng ty TNHH?",
  "top_k": 30
}
```

**Response:**
```json
{
  "answer": "D·ª±a v√†o Lu·∫≠t Doanh Nghi·ªáp 2020, ƒêi·ªÅu 4...",
  "sources": [
    {
      "doc_name": "Lu·∫≠t Doanh Nghi·ªáp 2020",
      "article": "ƒêi·ªÅu 4",
      "distance": 0.18
    }
  ]
}
```

### **2. Tax Calculation**

```http
POST /api/legal/calculate-tax
Content-Type: application/json

{
  "gross_salary": 50000000,
  "dependents": 2
}
```

**Response:**
```json
{
  "gross_salary": 50000000,
  "insurance": {
    "bhxh": 3744000,
    "bhyt": 702000,
    "bhtn": 500000,
    "total": 4946000
  },
  "deductions": {
    "personal": 11000000,
    "dependents": 8800000,
    "total": 24746000
  },
  "taxable_income": 25254000,
  "tax": 2525400,
  "net_salary": 42528600
}
```

---

## üß™ Testing

### **1. Test Accuracy:**

```bash
python scripts/test_current_system.py
```

**Output:**
```
================================================================================
üìä T·ªîNG K·∫æT TEST
================================================================================

K·∫øt qu·∫£:
  - T·ªïng s·ªë test: 5
  - T√¨m th·∫•y k·∫øt qu·∫£: 5/5 (100.0%)
  - ƒê√∫ng vƒÉn b·∫£n: 5/5 (100.0%)
  - ƒê√∫ng ƒëi·ªÅu: 2/5 (40.0%)
  - Ho√†n to√†n ch√≠nh x√°c: 2/5 (40.0%)
```

### **2. Test Search:**

```python
from services.legal.vector_service import LegalVectorService

vector_service = LegalVectorService()
results = vector_service.search(
    query="Ng∆∞·ªùi ƒë·∫°i di·ªán theo ph√°p lu·∫≠t",
    top_k=5
)

for r in results:
    print(f"{r['metadata']['doc_name']} - {r['metadata']['article']}")
```

### **3. Check VectorDB Stats:**

```python
stats = vector_service.get_collection_stats()
print(stats)
# {'total_chunks': 1621, 'unique_docs': 7}
```

---

## üêõ Troubleshooting

### **Problem 1: VectorDB Empty**

**Symptoms:**
```
Total chunks in DB: 0
```

**Solution:**
```bash
# Re-embed data
python scripts/embed_from_json.py --clear
```

---

### **Problem 2: Wrong Document Names**

**Symptoms:**
```
Doc name: "LU·∫¨T\nD" (corrupt)
```

**Solution:**
1. Check `parser.py` ‚Üí `extract_doc_name()`
2. Update `guess_law_name_from_filename()` mapping
3. Re-parse and re-embed

---

### **Problem 3: Low Accuracy**

**Symptoms:**
```
ƒê√∫ng ƒëi·ªÅu: 1/5 (20%)
```

**Solutions:**
1. **Increase top_k:** 20 ‚Üí 30 (done)
2. **Implement LLM re-ranking:** (30 ph√∫t)
3. **Hybrid search (BM25 + Vector):** (1-2 gi·ªù)
4. **Fine-tune embedding model:** (1-2 tu·∫ßn)

---

### **Problem 4: OOM During Embedding**

**Symptoms:**
```
Out of memory at batch 10
```

**Solution:**
- System t·ª± ƒë·ªông gi·∫£m batch_size
- N·∫øu v·∫´n l·ªói: Gi·∫£m `embedding_batch_size` trong `embed_from_json.py`

---

## üìö T√†i Li·ªáu Tham Kh·∫£o

### **VƒÉn B·∫£n Ph√°p Lu·∫≠t:**
- Lu·∫≠t Doanh Nghi·ªáp 2020 (67/2020/QH14)
- B·ªô Lu·∫≠t Lao ƒê·ªông 2019 (45/2019/QH14)
- Lu·∫≠t ƒê·∫ßu T∆∞ 2020 (61/2020/QH14)
- Lu·∫≠t Thu·∫ø GTGT 2024 (48/2024/QH15)
- Lu·∫≠t Thu·∫ø TNDN 2008 (14/2008/QH12)
- Lu·∫≠t Thu·∫ø TNCN 2007 (04/2007/QH12)

### **Technical Docs:**
- [ChromaDB Documentation](https://docs.trychroma.com/)
- [Sentence Transformers](https://www.sbert.net/)
- [Google Gemini API](https://ai.google.dev/docs)

---

## üë• Contributors

- **Tan Dat** - Developer

---

## üìù License

Private - E-commerce Project

---

**Last Updated:** 2026-01-12  
**Version:** 1.0  
**Status:** ‚úÖ Production Ready