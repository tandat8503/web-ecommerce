#!/usr/bin/env python3
"""
Process Massive Data - X·ª≠ l√Ω d·ªØ li·ªáu massive (1000-2000 reviews)
"""

import os
import sqlite3
import pandas as pd
import re
from pathlib import Path
from typing import List, Dict, Any
import sys

# Add parent directory to path
sys.path.append(str(Path(__file__).parent.parent))

from scripts.train_advanced_sentiment import (
    normalize_vietnamese_text, 
    expand_vietnamese_slang, 
    expand_underscore_words,
    extract_sentiment_features,
    LABEL_MAP
)

class MassiveDataProcessor:
    """X·ª≠ l√Ω d·ªØ li·ªáu massive"""
    
    def __init__(self, db_path: str = "data/sentiment_training.db"):
        self.db_path = db_path
        
    def load_massive_data(self, csv_path: str = "data/massive_reviews.csv") -> pd.DataFrame:
        """Load d·ªØ li·ªáu massive t·ª´ CSV file"""
        print("üîÑ Loading massive data from CSV...")
        
        try:
            if not os.path.exists(csv_path):
                print(f"‚ö†Ô∏è CSV file not found: {csv_path}")
                return pd.DataFrame()
            
            # Load t·ª´ CSV
            df_massive = pd.read_csv(csv_path, encoding='utf-8')
            
            print(f"‚úÖ Loaded {len(df_massive)} massive reviews from CSV")
            return df_massive
            
        except Exception as e:
            print(f"‚ùå Error loading massive data: {e}")
            return pd.DataFrame()
    
    def preprocess_massive_text(self, text: str) -> str:
        """Preprocess text t·ª´ massive data"""
        if not text or pd.isna(text):
            return ""
        
        # Chu·∫©n h√≥a vƒÉn b·∫£n
        text = normalize_vietnamese_text(text)
        
        # M·ªü r·ªông t·ª´ vi·∫øt t·∫Øt
        text = expand_vietnamese_slang(text)
        
        return text
    
    def validate_sentiment(self, sentiment: str, rating: int) -> str:
        """Validate v√† chu·∫©n h√≥a sentiment"""
        if pd.isna(sentiment):
            return "neutral"
        
        sentiment = sentiment.lower().strip()
        
        # N·∫øu c√≥ rating, s·ª≠ d·ª•ng rating ƒë·ªÉ x√°c ƒë·ªãnh sentiment
        if rating and rating > 0:
            if rating >= 4:
                return "positive"
            elif rating <= 2:
                return "negative"
            else:
                return "neutral"
        
        # N·∫øu kh√¥ng c√≥ rating, s·ª≠ d·ª•ng sentiment text
        if sentiment in ["positive", "pos", "t√≠ch c·ª±c", "t·ªët", "hay"]:
            return "positive"
        elif sentiment in ["negative", "neg", "ti√™u c·ª±c", "t·ªá", "d·ªü"]:
            return "negative"
        else:
            return "neutral"
    
    def filter_quality_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """L·ªçc d·ªØ li·ªáu ch·∫•t l∆∞·ª£ng cao"""
        print("üîç Filtering quality data...")
        
        original_count = len(df)
        
        # Lo·∫°i b·ªè d·ªØ li·ªáu tr·ªëng
        df = df.dropna(subset=['text'])
        df = df[df['text'].str.strip() != '']
        
        # Preprocess text
        df['processed_text'] = df['text'].apply(self.preprocess_massive_text)
        
        # Lo·∫°i b·ªè text qu√° ng·∫Øn ho·∫∑c kh√¥ng c√≥ √Ω nghƒ©a
        df = df[df['processed_text'].str.len() >= 10]
        df = df[df['processed_text'] != '']
        
        # Validate sentiment
        df['validated_sentiment'] = df.apply(
            lambda row: self.validate_sentiment(row['sentiment'], row['rating']), 
            axis=1
        )
        
        # Lo·∫°i b·ªè text c√≥ qu√° nhi·ªÅu k√Ω t·ª± ƒë·∫∑c bi·ªát
        df = df[df['processed_text'].str.count(r'[^\w\s]') / df['processed_text'].str.len() < 0.3]
        
        # Lo·∫°i b·ªè text c√≥ qu√° nhi·ªÅu s·ªë
        df = df[df['processed_text'].str.count(r'\d') / df['processed_text'].str.len() < 0.5]
        
        # Lo·∫°i b·ªè duplicate content
        df = df.drop_duplicates(subset=['processed_text'])
        
        filtered_count = len(df)
        print(f"‚úÖ Filtered from {original_count} to {filtered_count} reviews ({filtered_count/original_count*100:.1f}% kept)")
        
        return df
    
    def balance_massive_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """C√¢n b·∫±ng d·ªØ li·ªáu massive"""
        print("‚öñÔ∏è Balancing massive data by sentiment...")
        
        sentiment_counts = df['validated_sentiment'].value_counts()
        print(f"Original distribution: {sentiment_counts.to_dict()}")
        
        # T√¨m s·ªë l∆∞·ª£ng nh·ªè nh·∫•t
        min_count = sentiment_counts.min()
        
        # C√¢n b·∫±ng d·ªØ li·ªáu
        balanced_dfs = []
        for sentiment in sentiment_counts.index:
            sentiment_df = df[df['validated_sentiment'] == sentiment]
            if len(sentiment_df) > min_count:
                # Undersample n·∫øu qu√° nhi·ªÅu
                balanced_df = sentiment_df.sample(min_count, random_state=42)
            else:
                balanced_df = sentiment_df
            balanced_dfs.append(balanced_df)
        
        df_balanced = pd.concat(balanced_dfs, ignore_index=True)
        
        new_counts = df_balanced['validated_sentiment'].value_counts()
        print(f"Balanced distribution: {new_counts.to_dict()}")
        
        return df_balanced
    
    def integrate_with_training_data(self, df_massive: pd.DataFrame) -> None:
        """T√≠ch h·ª£p d·ªØ li·ªáu massive v√†o training data"""
        print("üîÑ Integrating massive data with training data...")
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ insert
            rows = []
            for _, row in df_massive.iterrows():
                # Tr√≠ch xu·∫•t features
                features = extract_sentiment_features(row['processed_text'])
                features_json = str(features)
                
                rows.append((
                    row['processed_text'],  # text
                    row['text'],         # original_text
                    LABEL_MAP[row['validated_sentiment']],  # label
                    f"massive_{row['source']}",  # source_file
                    1.0,                    # confidence
                    features_json           # preprocessing_info
                ))
            
            # Insert v√†o b·∫£ng training data
            cursor.executemany("""
                INSERT INTO sentiment_training_data 
                (text, original_text, label, source_file, confidence, preprocessing_info)
                VALUES (?, ?, ?, ?, ?, ?)
            """, rows)
            
            conn.commit()
            conn.close()
            
            print(f"‚úÖ Integrated {len(rows)} massive reviews into training data")
            
        except Exception as e:
            print(f"‚ùå Error integrating massive data: {e}")
    
    def generate_additional_neutral_data(self, count: int = 500) -> pd.DataFrame:
        """T·∫°o th√™m d·ªØ li·ªáu neutral t·ª´ templates"""
        print(f"üîÑ Generating {count} additional neutral samples...")
        
        # Templates cho neutral reviews
        neutral_templates = [
            "S·∫£n ph·∫©m {product} b√¨nh th∆∞·ªùng, kh√¥ng c√≥ g√¨ ƒë·∫∑c bi·ªát",
            "ƒê√∫ng v·ªõi m√¥ t·∫£, giao h√†ng ƒë√∫ng h·∫πn",
            "Gi√° c·∫£ h·ª£p l√Ω, ch·∫•t l∆∞·ª£ng ·ªïn",
            "S·∫£n ph·∫©m {product} ok, t·∫°m ƒë∆∞·ª£c",
            "Kh√¥ng c√≥ g√¨ n·ªïi b·∫≠t, c≈©ng kh√¥ng t·ªá",
            "S·∫£n ph·∫©m {product} b√¨nh th∆∞·ªùng th√¥i",
            "ƒê√∫ng nh∆∞ h√¨nh, kh√¥ng c√≥ g√¨ ƒë·∫∑c bi·ªát",
            "Gi√° v·∫≠y th√¨ c≈©ng ƒë∆∞·ª£c, kh√¥ng mong ƒë·ª£i g√¨ h∆°n",
            "S·∫£n ph·∫©m {product} ·ªïn, kh√¥ng c√≥ g√¨ ph√†n n√†n",
            "B√¨nh th∆∞·ªùng, kh√¥ng c√≥ g√¨ ƒë·∫∑c bi·ªát",
            "ƒê√∫ng v·ªõi m√¥ t·∫£, kh√¥ng c√≥ g√¨ n·ªïi b·∫≠t",
            "S·∫£n ph·∫©m {product} t·∫°m ƒë∆∞·ª£c, kh√¥ng c√≥ g√¨ ƒë·∫∑c bi·ªát",
            "Gi√° c·∫£ ph√π h·ª£p, ch·∫•t l∆∞·ª£ng b√¨nh th∆∞·ªùng",
            "S·∫£n ph·∫©m {product} ok, kh√¥ng c√≥ g√¨ ph√†n n√†n",
            "ƒê√∫ng nh∆∞ h√¨nh, kh√¥ng c√≥ g√¨ ƒë·∫∑c bi·ªát",
            "S·∫£n ph·∫©m {product} b√¨nh th∆∞·ªùng, kh√¥ng c√≥ g√¨ n·ªïi b·∫≠t",
            "Gi√° v·∫≠y th√¨ c≈©ng ƒë∆∞·ª£c, kh√¥ng mong ƒë·ª£i g√¨ h∆°n",
            "S·∫£n ph·∫©m {product} ·ªïn, kh√¥ng c√≥ g√¨ ph√†n n√†n",
            "B√¨nh th∆∞·ªùng, kh√¥ng c√≥ g√¨ ƒë·∫∑c bi·ªát",
            "ƒê√∫ng v·ªõi m√¥ t·∫£, kh√¥ng c√≥ g√¨ n·ªïi b·∫≠t"
        ]
        
        # Danh s√°ch s·∫£n ph·∫©m n·ªôi th·∫•t
        products = [
            "b√†n l√†m vi·ªác", "gh·∫ø vƒÉn ph√≤ng", "t·ªß s√°ch", "b√†n ƒÉn", 
            "gh·∫ø ƒÉn", "sofa", "gi∆∞·ªùng", "t·ªß qu·∫ßn √°o", "k·ªá s√°ch",
            "b√†n h·ªçc", "gh·∫ø xoay", "t·ªß gi√†y", "b√†n tr√†", "gh·∫ø sofa",
            "b√†n m√°y t√≠nh", "gh·∫ø gaming", "t·ªß tivi", "b√†n trang ƒëi·ªÉm",
            "gh·∫ø ƒë·ªçc s√°ch", "k·ªá tivi", "b√†n c√† ph√™", "gh·∫ø bar",
            "t·ªß b·∫øp", "b√†n ƒÉn g·ªó", "gh·∫ø ƒÉn g·ªó", "sofa g·ªó",
            "gi∆∞·ªùng g·ªó", "t·ªß qu·∫ßn √°o g·ªó", "k·ªá s√°ch g·ªó", "b√†n h·ªçc g·ªó"
        ]
        
        synthetic_data = []
        
        for i in range(count):
            # Ch·ªçn template ng·∫´u nhi√™n
            template = neutral_templates[i % len(neutral_templates)]
            product = products[i % len(products)]
            
            # T·∫°o text
            text = template.format(product=product)
            
            # Preprocess
            processed_text = self.preprocess_massive_text(text)
            
            # Tr√≠ch xu·∫•t features
            features = extract_sentiment_features(processed_text)
            
            synthetic_data.append({
                'text': text,
                'processed_text': processed_text,
                'validated_sentiment': 'neutral',
                'source': 'additional_neutral',
                'product_name': product,
                'features': features
            })
        
        df_synthetic = pd.DataFrame(synthetic_data)
        print(f"‚úÖ Generated {len(df_synthetic)} additional neutral samples")
        
        return df_synthetic
    
    def process_massive_data(self, csv_path: str = "data/massive_reviews.csv"):
        """X·ª≠ l√Ω d·ªØ li·ªáu massive"""
        print("üöÄ Starting massive data processing...")
        print("=" * 60)
        
        # 1. Load massive data t·ª´ CSV
        df_massive = self.load_massive_data(csv_path)
        
        if len(df_massive) == 0:
            print("‚ö†Ô∏è No massive data found. Generating synthetic data...")
            df_massive = self.generate_additional_neutral_data(1000)
        else:
            # 2. Filter quality data
            df_massive = self.filter_quality_data(df_massive)
            
            # 3. Balance data
            df_massive = self.balance_massive_data(df_massive)
        
        # 4. Generate additional neutral data
        df_additional = self.generate_additional_neutral_data(300)
        
        # 5. Combine data
        df_combined = pd.concat([df_massive, df_additional], ignore_index=True)
        
        # 6. Integrate with training data
        self.integrate_with_training_data(df_combined)
        
        # 7. Show statistics
        self.show_statistics()
        
        print("\nüéâ Massive data processing completed!")
    
    def show_statistics(self):
        """Hi·ªÉn th·ªã th·ªëng k√™ d·ªØ li·ªáu"""
        print("\nüìä Final Data Statistics:")
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Th·ªëng k√™ training data
            cursor.execute("""
                SELECT label, COUNT(*) as count 
                FROM sentiment_training_data 
                GROUP BY label
            """)
            
            results = cursor.fetchall()
            total = sum(count for _, count in results)
            
            print(f"Total training samples: {total}")
            print("Sentiment distribution:")
            
            label_names = {0: "negative", 1: "neutral", 2: "positive"}
            for label, count in results:
                percentage = count / total * 100
                print(f"  {label_names.get(label, 'unknown')}: {count} ({percentage:.1f}%)")
            
            # Th·ªëng k√™ sources
            cursor.execute("""
                SELECT source_file, COUNT(*) as count 
                FROM sentiment_training_data 
                GROUP BY source_file
                ORDER BY count DESC
            """)
            
            sources = cursor.fetchall()
            print("\nSource distribution:")
            for source, count in sources:
                percentage = count / total * 100
                print(f"  {source}: {count} ({percentage:.1f}%)")
            
            conn.close()
            
        except Exception as e:
            print(f"‚ùå Error showing statistics: {e}")

def main():
    """Main function"""
    processor = MassiveDataProcessor()
    processor.process_massive_data()

if __name__ == "__main__":
    main()
