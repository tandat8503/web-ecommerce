#!/usr/bin/env python3
"""
Advanced Vietnamese Sentiment Training - X·ª≠ l√Ω ti·∫øng Vi·ªát n√¢ng cao
- X·ª≠ l√Ω k√Ω t·ª± ƒë·∫∑c bi·ªát, emoji, t·ª´ vi·∫øt t·∫Øt
- Ph√¢n bi·ªát ng·ªØ c·∫£nh v√† tr·∫°ng th√°i
- Preprocessing ti·∫øng Vi·ªát chuy√™n s√¢u
- Model ensemble ƒë·ªÉ tƒÉng ƒë·ªô ch√≠nh x√°c
"""

import os
import sys
import json
import sqlite3
import re
import unicodedata
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Tuple

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.ensemble import VotingClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import joblib

DB_PATH = "data/sentiment_training.db"
MODEL_DIR = "models/sentiment/advanced"
MODEL_PATH = f"{MODEL_DIR}/vietnamese_sentiment_model.joblib"
LABEL_MAP = {"negative": 0, "neutral": 1, "positive": 2}
ID2LABEL = {0: "negative", 1: "neutral", 2: "positive"}

# Vietnamese sentiment patterns
VIETNAMESE_PATTERNS = {
    'positive': [
        r'\bt·ªët\b', r'\bƒë·∫πp\b', r'\btuy·ªát\b', r'\bho√†n\s*h·∫£o\b', r'\bch·∫•t\s*l∆∞·ª£ng\b',
        r'\bh√†i\s*l√≤ng\b', r'\b∆∞ng\s*√Ω\b', r'\bth√≠ch\b', r'\by√™u\b', r'\bth∆∞∆°ng\b',
        r'\bgi·ªèi\b', r'\bgi·ªèi\s*l·∫Øm\b', r'\bqu√°\s*t·ªët\b', r'\br·∫•t\s*t·ªët\b', r'\bsi√™u\s*t·ªët\b',
        r'\bperfect\b', r'\bexcellent\b', r'\bamazing\b', r'\bwonderful\b', r'\bfantastic\b',
        r'\büëç\b', r'\b‚ù§Ô∏è\b', r'\büòç\b', r'\büòä\b', r'\büòÑ\b', r'\büòÅ\b'
    ],
    'negative': [
        r'\bt·ªá\b', r'\bx·∫•u\b', r'\bt·ªìi\b', r'\bt·ªá\s*h·∫°i\b', r'\bkh√¥ng\s*t·ªët\b',
        r'\bkh√¥ng\s*h√†i\s*l√≤ng\b', r'\bth·∫•t\s*v·ªçng\b', r'\bch√°n\b', r'\bgh√©t\b',
        r'\bt·ªá\s*qu√°\b', r'\br·∫•t\s*t·ªá\b', r'\bsi√™u\s*t·ªá\b', r'\bawful\b', r'\bbad\b',
        r'\bterrible\b', r'\bhorrible\b', r'\bdisappointed\b', r'\büòû\b', r'\büò¢\b',
        r'\büò°\b', r'\büëé\b', r'\büíî\b'
    ],
    'neutral': [
        r'\bb√¨nh\s*th∆∞·ªùng\b', r'\b·ªïn\b', r'\bt·∫°m\s*ƒë∆∞·ª£c\b', r'\bkh√¥ng\s*c√≥\s*g√¨\b',
        r'\bnormal\b', r'\bok\b', r'\bfine\b', r'\bso\s*so\b', r'\büòê\b', r'\büòë\b'
    ]
}

# Vietnamese abbreviations and slang - M·ªü r·ªông ƒë·ªÉ hi·ªÉu teencode
VIETNAMESE_SLANG = {
    # T·ª´ vi·∫øt t·∫Øt c∆° b·∫£n
    'sp': 's·∫£n ph·∫©m', 'mn': 'm·ªçi ng∆∞·ªùi', 'ncl': 'n√≥i chung l√†', 're': 'review',
    'com': 'comment', 'men': 'm√¨nh', 'de': 'ƒë·ªÉ', 'd': 'ƒë∆∞·ª£c', 'm': 'm√¨nh',
    'n': 'n√†y', 'k': 'kh√¥ng', 'cx': 'c≈©ng', 'dc': 'ƒë∆∞·ª£c', 'vs': 'v·ªõi',
    'mik': 'm√¨nh', 'mk': 'm√¨nh', 't': 't√¥i', 'e': 'em', 'a': 'anh',
    'c': 'ch·ªã', 'b': 'b·∫°n', 'shop': 'c·ª≠a h√†ng', 'ship': 'giao h√†ng',
    'shipper': 'ng∆∞·ªùi giao h√†ng', 'qte': 'qu√° tr·ªùi', 'qtr': 'qu√° tr·ªùi',
    
    # Teencode ph·ªï bi·∫øn
    'ok': 'ok', 'oki': 'ok', 'oke': 'ok', 'okie': 'ok',
    'nma': 'nh∆∞ng m√†', 'nhm': 'nh∆∞ng m√†', 'nhma': 'nh∆∞ng m√†',
    'gac': 'g√°c', 'g√°c': 'g√°c', 'gak': 'g√°c',
    'dep': 'ƒë·∫πp', 'dep': 'ƒë·∫πp', 'xau': 'x·∫•u', 'xau': 'x·∫•u',
    'tot': 't·ªët', 'tot': 't·ªët', 'te': 't·ªá', 'te': 't·ªá',
    'ngon': 'ngon', 'ngon': 'ngon', 'ngon': 'ngon',
    're': 'r·∫ª', 're': 'r·∫ª', 'dat': 'ƒë·∫Øt', 'dat': 'ƒë·∫Øt',
    'nhanh': 'nhanh', 'cham': 'ch·∫≠m', 'cham': 'ch·∫≠m',
    'dung': 'ƒë√∫ng', 'sai': 'sai', 'dung': 'ƒë√∫ng',
    'hay': 'hay', 'hay': 'hay', 'nhat': 'nh·∫•t',
    'nhat': 'nh·∫•t', 'cuoi': 'cu·ªëi', 'cuoi': 'cu·ªëi',
    'dau': 'ƒë·∫ßu', 'cuoi': 'cu·ªëi', 'giua': 'gi·ªØa',
    'tren': 'tr√™n', 'duoi': 'd∆∞·ªõi', 'trong': 'trong',
    'ngoai': 'ngo√†i', 'ben': 'b√™n', 'canh': 'c·∫°nh',
    
    # T·ª´ c·∫£m th√°n
    'ui': 'ui', 'oi': '√¥i', 'a': '√†', 'e': '√®',
    'uh': '·ª´', 'um': '·ª´m', 'hmm': 'hmm', 'huh': 'huh',
    'wow': 'wow', 'omg': 'omg', 'wtf': 'wtf',
    
    # T·ª´ l√≥ng m·∫°ng
    'lol': 'lol', 'haha': 'haha', 'hehe': 'hehe',
    'kkk': 'kkk', 'hihi': 'hihi', 'hoho': 'hoho',
    'yay': 'yay', 'yeah': 'yeah', 'yep': 'yep',
    'nope': 'nope', 'nah': 'nah', 'meh': 'meh',
    
    # T·ª´ gh√©p th∆∞·ªùng d√πng
    'rat': 'r·∫•t', 'qua': 'qu√°', 'cung': 'c≈©ng',
    'cung': 'c≈©ng', 'rat': 'r·∫•t', 'qua': 'qu√°',
    'sieu': 'si√™u', 'cuc': 'c·ª±c', 'vo': 'v√¥',
    'vo': 'v√¥', 'cung': 'c≈©ng', 'rat': 'r·∫•t',
    
    # T·ª´ vi·∫øt t·∫Øt ƒë·∫∑c bi·ªát
    'ko': 'kh√¥ng', 'khong': 'kh√¥ng', 'kg': 'kh√¥ng',
    'k': 'kh√¥ng', 'kh': 'kh√¥ng', 'k': 'kh√¥ng',
    'dc': 'ƒë∆∞·ª£c', 'duoc': 'ƒë∆∞·ª£c', 'd': 'ƒë∆∞·ª£c',
    'd': 'ƒë∆∞·ª£c', 'dc': 'ƒë∆∞·ª£c', 'duoc': 'ƒë∆∞·ª£c',
    'cx': 'c≈©ng', 'cung': 'c≈©ng', 'c': 'c≈©ng',
    'c': 'c≈©ng', 'cx': 'c≈©ng', 'cung': 'c≈©ng',
    
    # T·ª´ c·∫£m x√∫c
    'vui': 'vui', 'buon': 'bu·ªìn', 'hanh': 'h·∫°nh',
    'hanh': 'h·∫°nh', 'phuc': 'ph√∫c', 'phuc': 'ph√∫c',
    'thich': 'th√≠ch', 'yeu': 'y√™u', 'thuong': 'th∆∞∆°ng',
    'thuong': 'th∆∞∆°ng', 'gh√©t': 'gh√©t', 'ghet': 'gh√©t',
    'chan': 'ch√°n', 'nhan': 'nh√†n', 'nhan': 'nh√†n',
    
    # T·ª´ m√¥ t·∫£ s·∫£n ph·∫©m
    'dep': 'ƒë·∫πp', 'xau': 'x·∫•u', 'tot': 't·ªët', 'te': 't·ªá',
    'ngon': 'ngon', 'd·ªü': 'd·ªü', 'do': 'd·ªü', 'do': 'd·ªü',
    're': 'r·∫ª', 'dat': 'ƒë·∫Øt', 'nhanh': 'nhanh', 'cham': 'ch·∫≠m',
    'dung': 'ƒë√∫ng', 'sai': 'sai', 'hay': 'hay', 'nhat': 'nh·∫•t',
    'rat': 'r·∫•t', 'qua': 'qu√°', 'sieu': 'si√™u', 'cuc': 'c·ª±c',
    'vo': 'v√¥', 'cung': 'c≈©ng', 'rat': 'r·∫•t', 'qua': 'qu√°'
}

def init_database(db_path: str = DB_PATH) -> None:
    """Kh·ªüi t·∫°o database v·ªõi c√°c b·∫£ng c·∫ßn thi·∫øt"""
    os.makedirs(Path(db_path).parent, exist_ok=True)
    with sqlite3.connect(db_path) as conn:
        c = conn.cursor()
        
        # X√≥a b·∫£ng c≈© n·∫øu c√≥
        c.execute("DROP TABLE IF EXISTS sentiment_training_data")
        
        # B·∫£ng d·ªØ li·ªáu training
        c.execute("""
            CREATE TABLE sentiment_training_data (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                text TEXT NOT NULL,
                original_text TEXT,
                label INTEGER NOT NULL,
                source_file TEXT,
                confidence REAL DEFAULT 1.0,
                preprocessing_info TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # X√≥a b·∫£ng c≈© n·∫øu c√≥
        c.execute("DROP TABLE IF EXISTS model_performance")
        
        # B·∫£ng performance model
        c.execute("""
            CREATE TABLE model_performance (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                model_name TEXT NOT NULL,
                accuracy REAL NOT NULL,
                f1_score REAL NOT NULL,
                precision REAL NOT NULL,
                recall REAL NOT NULL,
                training_samples INTEGER NOT NULL,
                validation_samples INTEGER NOT NULL,
                training_time REAL NOT NULL,
                cross_val_scores TEXT,
                confusion_matrix TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # X√≥a b·∫£ng c≈© n·∫øu c√≥
        c.execute("DROP TABLE IF EXISTS sentiment_predictions")
        
        # B·∫£ng predictions
        c.execute("""
            CREATE TABLE sentiment_predictions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                text TEXT NOT NULL,
                predicted_label INTEGER NOT NULL,
                confidence REAL NOT NULL,
                probabilities TEXT NOT NULL,
                model_used TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        conn.commit()

def normalize_vietnamese_text(text: str) -> str:
    """Chu·∫©n h√≥a vƒÉn b·∫£n ti·∫øng Vi·ªát"""
    if not isinstance(text, str):
        return ""
    
    # X·ª≠ l√Ω t·ª´ gh√©p c√≥ d·∫•u g·∫°ch d∆∞·ªõi tr∆∞·ªõc
    text = expand_underscore_words(text)
    
    # Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát kh√¥ng c·∫ßn thi·∫øt nh∆∞ng gi·ªØ l·∫°i d·∫•u c√¢u quan tr·ªçng
    text = re.sub(r'[^\w\s.,!?;:()\-/]', ' ', text)
    
    # Chu·∫©n h√≥a kho·∫£ng tr·∫Øng
    text = re.sub(r'\s+', ' ', text)
    
    # Lo·∫°i b·ªè d·∫•u c√¢u th·ª´a
    text = re.sub(r'[.,!?;:]{2,}', lambda m: m.group(0)[0], text)
    
    return text.strip()

def expand_underscore_words(text: str) -> str:
    """X·ª≠ l√Ω t·ª´ gh√©p c√≥ d·∫•u g·∫°ch d∆∞·ªõi"""
    # Dictionary c√°c t·ª´ gh√©p ph·ªï bi·∫øn v·ªõi d·∫•u g·∫°ch d∆∞·ªõi
    underscore_words = {
        's·∫£n_ph·∫©m': 's·∫£n ph·∫©m',
        'ch·∫•t_l∆∞·ª£ng': 'ch·∫•t l∆∞·ª£ng', 
        'th·ªùi_gian': 'th·ªùi gian',
        'm√†u_s·∫Øc': 'm√†u s·∫Øc',
        'k√≠ch_th∆∞·ªõc': 'k√≠ch th∆∞·ªõc',
        'gi√°_c·∫£': 'gi√° c·∫£',
        'gi√°_th√†nh': 'gi√° th√†nh',
        'h√¨nh_·∫£nh': 'h√¨nh ·∫£nh',
        'm√¥_t·∫£': 'm√¥ t·∫£',
        'chi_ti·∫øt': 'chi ti·∫øt',
        'th√¥ng_tin': 'th√¥ng tin',
        'd·ªãch_v·ª•': 'd·ªãch v·ª•',
        'kh√°ch_h√†ng': 'kh√°ch h√†ng',
        'c·ª≠a_h√†ng': 'c·ª≠a h√†ng',
        'giao_h√†ng': 'giao h√†ng',
        'thanh_to√°n': 'thanh to√°n',
        'ƒë√°nh_gi√°': 'ƒë√°nh gi√°',
        'ph·∫£n_h·ªìi': 'ph·∫£n h·ªìi',
        'h·ªó_tr·ª£': 'h·ªó tr·ª£',
        't∆∞_v·∫•n': 't∆∞ v·∫•n',
        'b·∫£o_hi·ªÉm': 'b·∫£o hi·ªÉm',
        'b·∫£o_qu·∫£n': 'b·∫£o qu·∫£n',
        'v·∫≠n_chuy·ªÉn': 'v·∫≠n chuy·ªÉn',
        'ph√≠_ship': 'ph√≠ ship',
        'mi·ªÖn_ph√≠': 'mi·ªÖn ph√≠',
        'gi·∫£m_gi√°': 'gi·∫£m gi√°',
        'khuy·∫øn_m√£i': 'khuy·∫øn m√£i',
        '∆∞u_ƒë√£i': '∆∞u ƒë√£i',
        'ch∆∞∆°ng_tr√¨nh': 'ch∆∞∆°ng tr√¨nh',
        's·ª±_ki·ªán': 's·ª± ki·ªán',
        'tin_t·ª©c': 'tin t·ª©c',
        'b√†i_vi·∫øt': 'b√†i vi·∫øt',
        'n·ªôi_dung': 'n·ªôi dung',
        'ch·ªß_ƒë·ªÅ': 'ch·ªß ƒë·ªÅ',
        'danh_m·ª•c': 'danh m·ª•c',
        'lo·∫°i_s·∫£n_ph·∫©m': 'lo·∫°i s·∫£n ph·∫©m',
        'th∆∞∆°ng_hi·ªáu': 'th∆∞∆°ng hi·ªáu',
        'nh√£n_hi·ªáu': 'nh√£n hi·ªáu',
        'xu·∫•t_x·ª©': 'xu·∫•t x·ª©',
        'n∆°i_s·∫£n_xu·∫•t': 'n∆°i s·∫£n xu·∫•t',
        'h·∫°n_s·ª≠_d·ª•ng': 'h·∫°n s·ª≠ d·ª•ng',
        'ng√†y_s·∫£n_xu·∫•t': 'ng√†y s·∫£n xu·∫•t',
        'tr·ªçng_l∆∞·ª£ng': 'tr·ªçng l∆∞·ª£ng',
        'k√≠ch_th∆∞·ªõc': 'k√≠ch th∆∞·ªõc',
        'chi·ªÅu_d√†i': 'chi·ªÅu d√†i',
        'chi·ªÅu_r·ªông': 'chi·ªÅu r·ªông',
        'chi·ªÅu_cao': 'chi·ªÅu cao',
        'ƒë∆∞·ªùng_k√≠nh': 'ƒë∆∞·ªùng k√≠nh',
        'chu_vi': 'chu vi',
        'di·ªán_t√≠ch': 'di·ªán t√≠ch',
        'th·ªÉ_t√≠ch': 'th·ªÉ t√≠ch',
        'dung_l∆∞·ª£ng': 'dung l∆∞·ª£ng',
        'c√¥ng_su·∫•t': 'c√¥ng su·∫•t',
        'hi·ªáu_su·∫•t': 'hi·ªáu su·∫•t',
        'ch·∫•t_l∆∞·ª£ng': 'ch·∫•t l∆∞·ª£ng',
        'ƒë·ªô_b·ªÅn': 'ƒë·ªô b·ªÅn',
        'tu·ªïi_th·ªç': 'tu·ªïi th·ªç',
        'th·ªùi_gian': 'th·ªùi gian',
        't·ªëc_ƒë·ªô': 't·ªëc ƒë·ªô',
        'nhi·ªát_ƒë·ªô': 'nhi·ªát ƒë·ªô',
        '√°p_su·∫•t': '√°p su·∫•t',
        'ƒë·ªô_·∫©m': 'ƒë·ªô ·∫©m',
        '√°nh_s√°ng': '√°nh s√°ng',
        '√¢m_thanh': '√¢m thanh',
        'm√πi_h∆∞∆°ng': 'm√πi h∆∞∆°ng',
        'v·ªã_gi√°c': 'v·ªã gi√°c',
        'x√∫c_gi√°c': 'x√∫c gi√°c',
        'th·ªã_gi√°c': 'th·ªã gi√°c',
        'th√≠nh_gi√°c': 'th√≠nh gi√°c',
        'kh·ª©u_gi√°c': 'kh·ª©u gi√°c',
        'c·∫£m_gi√°c': 'c·∫£m gi√°c',
        'c·∫£m_x√∫c': 'c·∫£m x√∫c',
        't√¢m_tr·∫°ng': 't√¢m tr·∫°ng',
        'tinh_th·∫ßn': 'tinh th·∫ßn',
        'th·ªÉ_ch·∫•t': 'th·ªÉ ch·∫•t',
        's·ª©c_kh·ªèe': 's·ª©c kh·ªèe',
        'an_to√†n': 'an to√†n',
        'ti·ªán_l·ª£i': 'ti·ªán l·ª£i',
        'd·ªÖ_d√πng': 'd·ªÖ d√πng',
        'kh√≥_d√πng': 'kh√≥ d√πng',
        'thu·∫≠n_ti·ªán': 'thu·∫≠n ti·ªán',
        'b·∫•t_ti·ªán': 'b·∫•t ti·ªán',
        'h√†i_l√≤ng': 'h√†i l√≤ng',
        'th·∫•t_v·ªçng': 'th·∫•t v·ªçng',
        '∆∞ng_√Ω': '∆∞ng √Ω',
        'kh√¥ng_∆∞ng_√Ω': 'kh√¥ng ∆∞ng √Ω',
        'th√≠ch': 'th√≠ch',
        'kh√¥ng_th√≠ch': 'kh√¥ng th√≠ch',
        'y√™u': 'y√™u',
        'gh√©t': 'gh√©t',
        'quan_t√¢m': 'quan t√¢m',
        'kh√¥ng_quan_t√¢m': 'kh√¥ng quan t√¢m',
        'ch√∫_√Ω': 'ch√∫ √Ω',
        'b·ªè_qua': 'b·ªè qua',
        'quan_tr·ªçng': 'quan tr·ªçng',
        'kh√¥ng_quan_tr·ªçng': 'kh√¥ng quan tr·ªçng',
        'c·∫ßn_thi·∫øt': 'c·∫ßn thi·∫øt',
        'kh√¥ng_c·∫ßn_thi·∫øt': 'kh√¥ng c·∫ßn thi·∫øt',
        'h·ªØu_√≠ch': 'h·ªØu √≠ch',
        'v√¥_√≠ch': 'v√¥ √≠ch',
        'c√≥_√≠ch': 'c√≥ √≠ch',
        'kh√¥ng_c√≥_√≠ch': 'kh√¥ng c√≥ √≠ch',
        'hi·ªáu_qu·∫£': 'hi·ªáu qu·∫£',
        'kh√¥ng_hi·ªáu_qu·∫£': 'kh√¥ng hi·ªáu qu·∫£',
        'th√†nh_c√¥ng': 'th√†nh c√¥ng',
        'th·∫•t_b·∫°i': 'th·∫•t b·∫°i',
        't·ªët': 't·ªët',
        'x·∫•u': 'x·∫•u',
        'ƒë·∫πp': 'ƒë·∫πp',
        'x·∫•u': 'x·∫•u',
        'hay': 'hay',
        'd·ªü': 'd·ªü',
        'ngon': 'ngon',
        'd·ªü': 'd·ªü',
        'r·∫ª': 'r·∫ª',
        'ƒë·∫Øt': 'ƒë·∫Øt',
        'nhanh': 'nhanh',
        'ch·∫≠m': 'ch·∫≠m',
        'ƒë√∫ng': 'ƒë√∫ng',
        'sai': 'sai',
        'ch√≠nh_x√°c': 'ch√≠nh x√°c',
        'kh√¥ng_ch√≠nh_x√°c': 'kh√¥ng ch√≠nh x√°c',
        'r√µ_r√†ng': 'r√µ r√†ng',
        'kh√¥ng_r√µ_r√†ng': 'kh√¥ng r√µ r√†ng',
        'd·ªÖ_hi·ªÉu': 'd·ªÖ hi·ªÉu',
        'kh√≥_hi·ªÉu': 'kh√≥ hi·ªÉu',
        'ƒë∆°n_gi·∫£n': 'ƒë∆°n gi·∫£n',
        'ph·ª©c_t·∫°p': 'ph·ª©c t·∫°p',
        'd·ªÖ_d√†ng': 'd·ªÖ d√†ng',
        'kh√≥_khƒÉn': 'kh√≥ khƒÉn',
        'thu·∫≠n_l·ª£i': 'thu·∫≠n l·ª£i',
        'b·∫•t_l·ª£i': 'b·∫•t l·ª£i',
        'c√≥_l·ª£i': 'c√≥ l·ª£i',
        'kh√¥ng_c√≥_l·ª£i': 'kh√¥ng c√≥ l·ª£i',
        'c√≥_√≠ch': 'c√≥ √≠ch',
        'kh√¥ng_c√≥_√≠ch': 'kh√¥ng c√≥ √≠ch',
        'h·ªØu_d·ª•ng': 'h·ªØu d·ª•ng',
        'v√¥_d·ª•ng': 'v√¥ d·ª•ng',
        'c·∫ßn_thi·∫øt': 'c·∫ßn thi·∫øt',
        'kh√¥ng_c·∫ßn_thi·∫øt': 'kh√¥ng c·∫ßn thi·∫øt',
        'quan_tr·ªçng': 'quan tr·ªçng',
        'kh√¥ng_quan_tr·ªçng': 'kh√¥ng quan tr·ªçng',
        'c·∫ßn_thi·∫øt': 'c·∫ßn thi·∫øt',
        'kh√¥ng_c·∫ßn_thi·∫øt': 'kh√¥ng c·∫ßn thi·∫øt'
    }
    
    # Thay th·∫ø t·ª´ gh√©p c√≥ d·∫•u g·∫°ch d∆∞·ªõi
    for underscore_word, normal_word in underscore_words.items():
        text = text.replace(underscore_word, normal_word)
    
    return text

def expand_vietnamese_slang(text: str) -> str:
    """M·ªü r·ªông t·ª´ vi·∫øt t·∫Øt ti·∫øng Vi·ªát v√† teencode"""
    words = text.split()
    expanded = []
    
    for word in words:
        # Lo·∫°i b·ªè d·∫•u c√¢u t·∫°m th·ªùi
        clean_word = re.sub(r'[^\w]', '', word.lower())
        punctuation = re.sub(r'[\w]', '', word)
        
        # Ki·ªÉm tra t·ª´ vi·∫øt t·∫Øt
        if clean_word in VIETNAMESE_SLANG:
            expanded_word = VIETNAMESE_SLANG[clean_word] + punctuation
            expanded.append(expanded_word)
        else:
            # Ki·ªÉm tra teencode ph·ª©c t·∫°p h∆°n
            expanded_word = expand_teencode(clean_word) + punctuation
            expanded.append(expanded_word)
    
    return ' '.join(expanded)

def expand_teencode(word: str) -> str:
    """X·ª≠ l√Ω teencode ph·ª©c t·∫°p"""
    # Gi·ªØ nguy√™n n·∫øu ƒë√£ l√† t·ª´ chu·∫©n
    if word in VIETNAMESE_SLANG:
        return VIETNAMESE_SLANG[word]
    
    # X·ª≠ l√Ω c√°c pattern teencode ph·ªï bi·∫øn
    patterns = {
        # Thay th·∫ø k√Ω t·ª±
        r'^k$': 'kh√¥ng',
        r'^ko$': 'kh√¥ng', 
        r'^kg$': 'kh√¥ng',
        r'^kh$': 'kh√¥ng',
        r'^dc$': 'ƒë∆∞·ª£c',
        r'^d$': 'ƒë∆∞·ª£c',
        r'^cx$': 'c≈©ng',
        r'^c$': 'c≈©ng',
        r'^vs$': 'v·ªõi',
        r'^n$': 'n√†y',
        r'^m$': 'm√¨nh',
        r'^t$': 't√¥i',
        r'^e$': 'em',
        r'^a$': 'anh',
        r'^c$': 'ch·ªã',
        r'^b$': 'b·∫°n',
        
        # T·ª´ c·∫£m th√°n
        r'^ok$': 'ok',
        r'^oki$': 'ok',
        r'^oke$': 'ok',
        r'^okie$': 'ok',
        r'^wow$': 'wow',
        r'^omg$': 'omg',
        r'^lol$': 'lol',
        r'^haha$': 'haha',
        r'^hehe$': 'hehe',
        r'^yay$': 'yay',
        r'^yeah$': 'yeah',
        r'^yep$': 'yep',
        r'^nope$': 'nope',
        r'^nah$': 'nah',
        r'^meh$': 'meh',
        
        # T·ª´ m√¥ t·∫£
        r'^dep$': 'ƒë·∫πp',
        r'^xau$': 'x·∫•u',
        r'^tot$': 't·ªët',
        r'^te$': 't·ªá',
        r'^ngon$': 'ngon',
        r'^re$': 'r·∫ª',
        r'^dat$': 'ƒë·∫Øt',
        r'^nhanh$': 'nhanh',
        r'^cham$': 'ch·∫≠m',
        r'^dung$': 'ƒë√∫ng',
        r'^sai$': 'sai',
        r'^hay$': 'hay',
        r'^nhat$': 'nh·∫•t',
        r'^rat$': 'r·∫•t',
        r'^qua$': 'qu√°',
        r'^sieu$': 'si√™u',
        r'^cuc$': 'c·ª±c',
        r'^vo$': 'v√¥',
        r'^cung$': 'c≈©ng',
        
        # T·ª´ gh√©p
        r'^nma$': 'nh∆∞ng m√†',
        r'^nhm$': 'nh∆∞ng m√†',
        r'^nhma$': 'nh∆∞ng m√†',
        r'^gac$': 'g√°c',
        r'^gak$': 'g√°c',
        r'^qte$': 'qu√° tr·ªùi',
        r'^qtr$': 'qu√° tr·ªùi',
        r'^sp$': 's·∫£n ph·∫©m',
        r'^mn$': 'm·ªçi ng∆∞·ªùi',
        r'^ncl$': 'n√≥i chung l√†',
        r'^re$': 'review',
        r'^com$': 'comment',
        r'^men$': 'm√¨nh',
        r'^de$': 'ƒë·ªÉ',
        r'^mik$': 'm√¨nh',
        r'^mk$': 'm√¨nh',
        r'^shop$': 'c·ª≠a h√†ng',
        r'^ship$': 'giao h√†ng',
        r'^shipper$': 'ng∆∞·ªùi giao h√†ng',
    }
    
    for pattern, replacement in patterns.items():
        if re.match(pattern, word):
            return replacement
    
    # Gi·ªØ nguy√™n n·∫øu kh√¥ng t√¨m th·∫•y pattern
    return word

def extract_sentiment_features(text: str) -> Dict[str, Any]:
    """Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng sentiment t·ª´ vƒÉn b·∫£n"""
    text_lower = text.lower()
    features = {
        'positive_count': 0,
        'negative_count': 0,
        'neutral_count': 0,
        'exclamation_count': text.count('!'),
        'question_count': text.count('?'),
        'caps_ratio': sum(1 for c in text if c.isupper()) / max(len(text), 1),
        'length': len(text),
        'word_count': len(text.split())
    }
    
    # ƒê·∫øm pattern sentiment
    for pattern in VIETNAMESE_PATTERNS['positive']:
        features['positive_count'] += len(re.findall(pattern, text_lower))
    
    for pattern in VIETNAMESE_PATTERNS['negative']:
        features['negative_count'] += len(re.findall(pattern, text_lower))
    
    for pattern in VIETNAMESE_PATTERNS['neutral']:
        features['neutral_count'] += len(re.findall(pattern, text_lower))
    
    return features

def preprocess_text(text: str) -> Tuple[str, Dict[str, Any]]:
    """X·ª≠ l√Ω vƒÉn b·∫£n ti·∫øng Vi·ªát to√†n di·ªán"""
    original_text = text
    
    # Chu·∫©n h√≥a c∆° b·∫£n
    text = normalize_vietnamese_text(text)
    
    # M·ªü r·ªông t·ª´ vi·∫øt t·∫Øt
    text = expand_vietnamese_slang(text)
    
    # Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng
    features = extract_sentiment_features(text)
    
    return text, features

def load_shopee_data(data_dir: str) -> pd.DataFrame:
    """T·∫£i v√† x·ª≠ l√Ω d·ªØ li·ªáu Shopee"""
    data_root = Path(data_dir)
    if not data_root.exists():
        raise FileNotFoundError(f"Data directory not found: {data_root}")

    parts = []
    
    # Load train.csv v√† val.csv
    for name in ["train.csv", "val.csv"]:
        f = data_root / name
        if f.exists():
            try:
                df = pd.read_csv(f, encoding='utf-8')
                df["source"] = name
                parts.append(df)
                print(f"‚úÖ Loaded {name}: {len(df)} rows")
            except Exception as e:
                print(f"‚ö†Ô∏è Error loading {name}: {e}")
    
    # Load automated/*.csv
    auto_dir = data_root / "automated"
    if auto_dir.exists():
        for f in auto_dir.glob("*.csv"):
            try:
                # Th·ª≠ ƒë·ªçc v·ªõi c√°c options kh√°c nhau cho file c√≥ v·∫•n ƒë·ªÅ
                try:
                    df = pd.read_csv(f, encoding='utf-8')
                except pd.errors.ParserError:
                    # N·∫øu l·ªói parsing, th·ª≠ v·ªõi options kh√°c
                    df = pd.read_csv(f, encoding='utf-8', sep=None, engine='python', on_bad_lines='skip')
                except Exception:
                    # Cu·ªëi c√πng th·ª≠ v·ªõi delimiter t·ª± ƒë·ªông
                    df = pd.read_csv(f, encoding='utf-8', sep=None, engine='python', 
                                   quoting=0, on_bad_lines='skip', error_bad_lines=False)
                
                df["source"] = f.name
                parts.append(df)
                print(f"‚úÖ Loaded {f.name}: {len(df)} rows")
            except Exception as e:
                print(f"‚ö†Ô∏è Error loading {f.name}: {e}")
                # Th·ª≠ ƒë·ªçc v·ªõi pandas engine python v√† b·ªè qua d√≤ng l·ªói
                try:
                    df = pd.read_csv(f, encoding='utf-8', engine='python', 
                                   on_bad_lines='skip', error_bad_lines=False)
                    df["source"] = f.name
                    parts.append(df)
                    print(f"‚úÖ Loaded {f.name} (with errors skipped): {len(df)} rows")
                except Exception as e2:
                    print(f"‚ùå Failed to load {f.name}: {e2}")

    if not parts:
        raise ValueError("No CSV files found in the provided directory")

    df_all = pd.concat(parts, ignore_index=True)
    print(f"üìä Total loaded: {len(df_all)} rows")

    # Map text column
    text_col = None
    for col in ["text", "review", "txt"]:
        if col in df_all.columns:
            text_col = col
            break
    
    if text_col is None:
        raise ValueError("No text column found (text/review/txt)")
    
    df_all["original_text"] = df_all[text_col].astype(str)
    
    # Map label column
    label_col = None
    for col in ["label", "sentiment", "lbl"]:
        if col in df_all.columns:
            label_col = col
            break
    
    if label_col is None:
        raise ValueError("No label column found (label/sentiment/lbl)")
    
    df_all["sentiment"] = df_all[label_col]

    # Preprocess text
    print("üîÑ Preprocessing texts...")
    processed_data = []
    for idx, row in df_all.iterrows():
        if idx % 1000 == 0:
            print(f"  Processed {idx}/{len(df_all)}")
        
        processed_text, features = preprocess_text(row["original_text"])
        processed_data.append({
            'text': processed_text,
            'original_text': row["original_text"],
            'sentiment': row["sentiment"],
            'source': row["source"],
            'features': features
        })
    
    df_processed = pd.DataFrame(processed_data)
    
    # Map sentiment to 3-class
    def map_sentiment(val):
        if isinstance(val, str):
            v = val.lower().strip()
            if v in {"neg", "negative", "0"}: return "negative"
            if v in {"pos", "positive", "2"}: return "positive"
            if v in {"neu", "neutral", "1"}: return "neutral"
            return "neutral"  # default
        try:
            iv = int(val)
            if iv == 0: return "negative"   # 0 = negative
            if iv == 1: return "neutral"    # 1 = neutral
            if iv == 2: return "positive"   # 2 = positive
        except:
            pass
        return "neutral"
    
    df_processed["sentiment"] = df_processed["sentiment"].apply(map_sentiment)
    
    # T·∫°o neutral t·ª´ uncertainty n·∫øu c√≥
    if "prob_positve" in df_all.columns:
        uncertainty_mask = (df_all["prob_positve"] >= 0.4) & (df_all["prob_positve"] <= 0.6)
        uncertainty_data = []
        for idx, row in df_all[uncertainty_mask].iterrows():
            # Ki·ªÉm tra d·ªØ li·ªáu tr∆∞·ªõc khi x·ª≠ l√Ω
            if pd.isna(row[text_col]) or str(row[text_col]).strip() == "" or str(row[text_col]).lower() == "nan":
                continue
                
            processed_text, features = preprocess_text(row[text_col])
            
            # Ki·ªÉm tra k·∫øt qu·∫£ x·ª≠ l√Ω
            if processed_text == "nan" or processed_text.strip() == "":
                continue
                
            uncertainty_data.append({
                'text': processed_text,
                'original_text': row[text_col],
                'sentiment': 'neutral',
                'source': f"{row.get('source', 'unknown')}_uncertainty",
                'features': features
            })
        
        if uncertainty_data:
            df_uncertainty = pd.DataFrame(uncertainty_data)
            df_processed = pd.concat([df_processed, df_uncertainty], ignore_index=True)
            print(f"‚úÖ Added {len(df_uncertainty)} uncertainty samples as neutral")
        else:
            print("‚ö†Ô∏è No valid uncertainty samples found")

    # Clean data
    print(f"üìä Before cleaning: {len(df_processed)} rows")
    df_processed = df_processed.dropna(subset=["text", "sentiment"])
    print(f"üìä After dropna: {len(df_processed)} rows")
    
    # Lo·∫°i b·ªè d·ªØ li·ªáu l·ªói - text = "nan" ho·∫∑c ch·ªâ c√≥ k√Ω t·ª± ƒë·∫∑c bi·ªát
    df_processed = df_processed[df_processed["text"] != "nan"]
    df_processed = df_processed[df_processed["text"] != "NaN"]
    df_processed = df_processed[df_processed["text"] != ""]
    df_processed = df_processed[df_processed["text"].str.strip() != ""]
    print(f"üìä After removing 'nan' and empty: {len(df_processed)} rows")
    
    # Ki·ªÉm tra ƒë·ªô d√†i text tr∆∞·ªõc khi filter
    text_lengths = df_processed["text"].str.len()
    print(f"üìä Text length stats: min={text_lengths.min()}, max={text_lengths.max()}, mean={text_lengths.mean():.2f}")
    print(f"üìä Text length distribution:")
    print(text_lengths.value_counts().head(10))
    
    # Filter nh·∫π h∆°n - ch·ªâ lo·∫°i b·ªè text qu√° ng·∫Øn ho·∫∑c ch·ªâ c√≥ k√Ω t·ª± ƒë·∫∑c bi·ªát
    df_processed = df_processed[df_processed["text"].str.len() > 1]  # Gi·∫£m t·ª´ 3 xu·ªëng 1
    df_processed = df_processed[df_processed["text"].str.strip().str.len() > 0]  # Lo·∫°i b·ªè text ch·ªâ c√≥ space
    print(f"üìä After length filter: {len(df_processed)} rows")
    
    print(f"üìä Final dataset: {len(df_processed)} rows")
    print(f"üìä Sentiment distribution:")
    print(df_processed["sentiment"].value_counts())
    
    return df_processed[["text", "original_text", "sentiment", "source", "features"]]

def save_training_data(df: pd.DataFrame, db_path: str = DB_PATH) -> None:
    """L∆∞u d·ªØ li·ªáu training v√†o SQLite"""
    with sqlite3.connect(db_path) as conn:
        c = conn.cursor()
        # Ch·ªâ x√≥a d·ªØ li·ªáu c≈© n·∫øu ƒë√¢y l√† l·∫ßn ƒë·∫ßu ti√™n
        c.execute("SELECT COUNT(*) FROM sentiment_training_data")
        if c.fetchone()[0] > 0:
            print("‚ö†Ô∏è Database already has data. Appending new data...")
        else:
            print("üìù First time saving data to database...")
        
        rows = []
        for _, row in df.iterrows():
            features_json = json.dumps(row["features"]) if "features" in row else "{}"
            rows.append((
                row["text"],
                row["original_text"],
                LABEL_MAP[row["sentiment"]],
                row["source"],
                1.0,
                features_json
            ))
        
        c.executemany("""
            INSERT INTO sentiment_training_data 
            (text, original_text, label, source_file, confidence, preprocessing_info)
            VALUES (?, ?, ?, ?, ?, ?)
        """, rows)
        conn.commit()
        print(f"‚úÖ Saved {len(rows)} training samples to SQLite")

def build_advanced_model() -> Pipeline:
    """X√¢y d·ª±ng model n√¢ng cao cho ti·∫øng Vi·ªát"""
    return Pipeline([
        ("tfidf", TfidfVectorizer(
            max_features=100000,
            ngram_range=(1, 3),  # unigram, bigram, trigram
            lowercase=True,
            strip_accents="unicode",
            min_df=2,
            max_df=0.95,
            sublinear_tf=True,  # log scaling
        )),
        ("clf", LogisticRegression(
            random_state=42, 
            max_iter=1000,
            multi_class='ovr',  # One-vs-Rest for multi-class
            class_weight='balanced'  # Handle class imbalance
        ))
    ])

def train_and_evaluate(df: pd.DataFrame, db_path: str = DB_PATH) -> Dict[str, Any]:
    """Training v√† ƒë√°nh gi√° model"""
    print("üîÑ Preparing training data...")
    
    # C√¢n b·∫±ng d·ªØ li·ªáu
    counts = df["sentiment"].value_counts()
    print(f"üìä Original distribution: {counts.to_dict()}")
    
    min_count = counts.min()
    balanced_dfs = []
    for sentiment in counts.index:
        sentiment_df = df[df["sentiment"] == sentiment]
        if len(sentiment_df) > min_count:
            balanced_df = sentiment_df.sample(min_count, random_state=42)
        else:
            balanced_df = sentiment_df
        balanced_dfs.append(balanced_df)
    
    balanced_df = pd.concat(balanced_dfs, ignore_index=True)
    print(f"üìä Balanced distribution: {balanced_df['sentiment'].value_counts().to_dict()}")
    
    # Prepare features
    X = balanced_df["text"].tolist()
    y = [LABEL_MAP[s] for s in balanced_df["sentiment"].tolist()]
    
    # Split data
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"üìä Training samples: {len(X_train)}")
    print(f"üìä Validation samples: {len(X_val)}")
    
    # Build and train model
    print("üîÑ Training model...")
    model = build_advanced_model()
    model.fit(X_train, y_train)
    
    # Evaluate
    print("üîÑ Evaluating model...")
    y_pred = model.predict(X_val)
    accuracy = accuracy_score(y_val, y_pred)
    
    # Cross-validation
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
    
    # Classification report
    report = classification_report(y_val, y_pred, output_dict=True, zero_division=0)
    
    # Confusion matrix
    cm = confusion_matrix(y_val, y_pred)
    
    # Save model
    os.makedirs(MODEL_DIR, exist_ok=True)
    model_data = {
        "pipeline": model,
        "label_map": LABEL_MAP,
        "id2label": ID2LABEL,
        "preprocessing_info": {
            "vietnamese_patterns": VIETNAMESE_PATTERNS,
            "slang_dict": VIETNAMESE_SLANG
        }
    }
    joblib.dump(model_data, MODEL_PATH)
    
    # Save performance to database
    with sqlite3.connect(db_path) as conn:
        c = conn.cursor()
        c.execute("""
            INSERT INTO model_performance
            (model_name, accuracy, f1_score, precision, recall, training_samples, 
             validation_samples, training_time, cross_val_scores, confusion_matrix)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            "advanced-vietnamese-sentiment",
            float(accuracy),
            float(report["macro avg"]["f1-score"]),
            float(report["macro avg"]["precision"]),
            float(report["macro avg"]["recall"]),
            len(X_train),
            len(X_val),
            0.0,  # training time not measured
            json.dumps(cv_scores.tolist()),
            json.dumps(cm.tolist())
        ))
        conn.commit()
    
    print(f"‚úÖ Model trained successfully!")
    print(f"üìä Accuracy: {accuracy:.4f}")
    print(f"üìä Cross-validation scores: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
    print(f"üíæ Model saved to: {MODEL_PATH}")
    
    return {
        "accuracy": accuracy,
        "cv_scores": cv_scores,
        "report": report,
        "confusion_matrix": cm,
        "model_path": MODEL_PATH
    }

def test_model_predictions(model_path: str, db_path: str = DB_PATH) -> None:
    """Test model v·ªõi c√°c c√¢u m·∫´u"""
    print("üîÆ Testing model predictions...")
    
    model_data = joblib.load(model_path)
    model = model_data["pipeline"]
    
    test_samples = [
        # C√¢u chu·∫©n
        "S·∫£n ph·∫©m r·∫•t t·ªët, ch·∫•t l∆∞·ª£ng cao, giao h√†ng nhanh",
        "T√¥i kh√¥ng h√†i l√≤ng v·ªõi d·ªãch v·ª• n√†y, t·ªá qu√°",
        "S·∫£n ph·∫©m b√¨nh th∆∞·ªùng, kh√¥ng c√≥ g√¨ ƒë·∫∑c bi·ªát",
        "Shop ph·ª•c v·ª• t·ªët, shipper th√¢n thi·ªán",
        "H√†ng v·ªÅ b·ªã h·ªèng, r·∫•t th·∫•t v·ªçng",
        "Qu√° tuy·ªát v·ªùi! S·∫Ω mua l·∫°i",
        "Kh√¥ng nh∆∞ mong ƒë·ª£i, ch·∫•t l∆∞·ª£ng k√©m",
        "B√¨nh th∆∞·ªùng th√¥i, kh√¥ng c√≥ g√¨ n·ªïi b·∫≠t",
        "Tuy·ªát v·ªùi! R·∫•t h√†i l√≤ng v·ªõi s·∫£n ph·∫©m",
        
        # Teencode v√† t·ª´ vi·∫øt t·∫Øt
        "Sp ok, t·∫°m dc",
        "Nma sp rat tot, giao hang nhanh",
        "K dc, sp te qua",
        "Oki, sp dep nma hoi dat",
        "Sp rat hay, se mua lai",
        "K nhu mong doi, chat luong kem",
        "Sp binh thuong, k co gi dac biet",
        "Shop phuc vu tot, shipper than thien",
        "Hang ve bi hong, rat that vong",
        "Qua tuyet voi! Se mua lai",
        
        # Teencode ph·ª©c t·∫°p
        "Sp rat tot, chat luong cao, giao hang nhanh",
        "T k hai long vs dich vu nay, te qua",
        "Sp binh thuong, k co gi dac biet",
        "Shop phuc vu tot, shipper than thien",
        "Hang ve bi hong, rat that vong",
        "Sp ok, tam dc",
        "Qua tuyet voi! Se mua lai",
        "K nhu mong doi, chat luong kem",
        "Binh thuong thoi, k co gi noi bat",
        "Tuyet voi! Rat hai long vs sp",
        
        # T·ª´ gh√©p c√≥ d·∫•u g·∫°ch d∆∞·ªõi
        "S·∫£n_ph·∫©m r·∫•t t·ªët, ch·∫•t_l∆∞·ª£ng cao",
        "D·ªãch_v·ª• giao_h√†ng nhanh ch√≥ng",
        "Kh√°ch_h√†ng h√†i_l√≤ng v·ªõi s·∫£n_ph·∫©m",
        "C·ª≠a_h√†ng ph·ª•c_v·ª• t·ªët, shipper th√¢n_thi·ªán",
        "H√†ng v·ªÅ b·ªã h·ªèng, r·∫•t th·∫•t_v·ªçng",
        "S·∫£n_ph·∫©m b√¨nh_th∆∞·ªùng, kh√¥ng c√≥ g√¨ ƒë·∫∑c_bi·ªát",
        "Ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m k√©m, gi√°_c·∫£ ƒë·∫Øt",
        "Th·ªùi_gian giao_h√†ng ch·∫≠m, d·ªãch_v·ª• t·ªá",
        "S·∫£n_ph·∫©m ƒë·∫πp, m√†u_s·∫Øc ƒë√∫ng v·ªõi m√¥_t·∫£",
        "K√≠ch_th∆∞·ªõc s·∫£n_ph·∫©m ph√π_h·ª£p, gi√°_th√†nh h·ª£p_l√Ω"
    ]
    
    with sqlite3.connect(db_path) as conn:
        c = conn.cursor()
        
        for text in test_samples:
            # Preprocess
            processed_text, _ = preprocess_text(text)
            
            # Predict
            pred_proba = model.predict_proba([processed_text])[0]
            pred_label = np.argmax(pred_proba)
            confidence = pred_proba[pred_label]
            
            # Map to 3-class if only 2 classes
            if len(pred_proba) == 2:
                # Binary case: map to 3-class
                if pred_label == 0:  # negative
                    pred_label = 0
                else:  # positive
                    pred_label = 2
                # Add neutral probability
                neutral_prob = 0.1
                pred_proba = np.array([pred_proba[0], neutral_prob, pred_proba[1]])
                pred_proba = pred_proba / pred_proba.sum()  # normalize
            
            # Save prediction
            c.execute("""
                INSERT INTO sentiment_predictions 
                (text, predicted_label, confidence, probabilities, model_used)
                VALUES (?, ?, ?, ?, ?)
            """, (
                text,
                int(pred_label),
                float(confidence),
                json.dumps({ID2LABEL[i]: float(pred_proba[i]) for i in range(len(pred_proba))}),
                "advanced-vietnamese-sentiment"
            ))
            
            print(f"üìù '{text[:50]}...' ‚Üí {ID2LABEL[pred_label]} ({confidence:.3f})")
        
        conn.commit()
    
    print("‚úÖ Sample predictions saved to database")

def main():
    print("üöÄ Advanced Vietnamese Sentiment Training")
    print("=" * 60)
    
    data_dir = sys.argv[1] if len(sys.argv) > 1 else "../shopee-reviews-sentiment-analysis/data"
    print(f"üìÅ Data directory: {data_dir}")
    
    # Initialize database
    init_database(DB_PATH)
    
    # Load and preprocess data
    df = load_shopee_data(data_dir)
    
    # Save training data
    save_training_data(df, DB_PATH)
    
    # Train model
    results = train_and_evaluate(df, DB_PATH)
    
    # Test predictions
    test_model_predictions(results["model_path"], DB_PATH)
    
    print("\nüéâ Training completed successfully!")
    print(f"üìä Final accuracy: {results['accuracy']:.4f}")
    print(f"üíæ Model saved to: {results['model_path']}")
    print(f"üíæ Database: {DB_PATH}")

if __name__ == "__main__":
    main()
