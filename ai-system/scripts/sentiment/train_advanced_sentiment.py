#!/usr/bin/env python3
"""
Advanced Vietnamese Sentiment Training - Xá»­ lÃ½ tiáº¿ng Viá»‡t nÃ¢ng cao
- Xá»­ lÃ½ kÃ½ tá»± Ä‘áº·c biá»‡t, emoji, tá»« viáº¿t táº¯t
- PhÃ¢n biá»‡t ngá»¯ cáº£nh vÃ  tráº¡ng thÃ¡i
- Preprocessing tiáº¿ng Viá»‡t chuyÃªn sÃ¢u
- Model ensemble Ä‘á»ƒ tÄƒng Ä‘á»™ chÃ­nh xÃ¡c
"""

import os
import sys
import json
import sqlite3
import re
import unicodedata
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Tuple

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.ensemble import VotingClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import joblib

DB_PATH = "data/sentiment_training.db"
MODEL_DIR = "models/sentiment/advanced"
MODEL_PATH = f"{MODEL_DIR}/vietnamese_sentiment_model.joblib"
LABEL_MAP = {"negative": 0, "neutral": 1, "positive": 2}
ID2LABEL = {0: "negative", 1: "neutral", 2: "positive"}

# Vietnamese sentiment patterns
VIETNAMESE_PATTERNS = {
    'positive': [
        r'\btá»‘t\b', r'\bÄ‘áº¹p\b', r'\btuyá»‡t\b', r'\bhoÃ n\s*háº£o\b', r'\bcháº¥t\s*lÆ°á»£ng\b',
        r'\bhÃ i\s*lÃ²ng\b', r'\bÆ°ng\s*Ã½\b', r'\bthÃ­ch\b', r'\byÃªu\b', r'\bthÆ°Æ¡ng\b',
        r'\bgiá»i\b', r'\bgiá»i\s*láº¯m\b', r'\bquÃ¡\s*tá»‘t\b', r'\bráº¥t\s*tá»‘t\b', r'\bsiÃªu\s*tá»‘t\b',
        r'\bperfect\b', r'\bexcellent\b', r'\bamazing\b', r'\bwonderful\b', r'\bfantastic\b',
        r'\bğŸ‘\b', r'\bâ¤ï¸\b', r'\bğŸ˜\b', r'\bğŸ˜Š\b', r'\bğŸ˜„\b', r'\bğŸ˜\b'
    ],
    'negative': [
        r'\btá»‡\b', r'\bxáº¥u\b', r'\btá»“i\b', r'\btá»‡\s*háº¡i\b', r'\bkhÃ´ng\s*tá»‘t\b',
        r'\bkhÃ´ng\s*hÃ i\s*lÃ²ng\b', r'\btháº¥t\s*vá»ng\b', r'\bchÃ¡n\b', r'\bghÃ©t\b',
        r'\btá»‡\s*quÃ¡\b', r'\bráº¥t\s*tá»‡\b', r'\bsiÃªu\s*tá»‡\b', r'\bawful\b', r'\bbad\b',
        r'\bterrible\b', r'\bhorrible\b', r'\bdisappointed\b', r'\bğŸ˜\b', r'\bğŸ˜¢\b',
        r'\bğŸ˜¡\b', r'\bğŸ‘\b', r'\bğŸ’”\b'
    ],
    'neutral': [
        r'\bbÃ¬nh\s*thÆ°á»ng\b', r'\bá»•n\b', r'\btáº¡m\s*Ä‘Æ°á»£c\b', r'\bkhÃ´ng\s*cÃ³\s*gÃ¬\b',
        r'\bnormal\b', r'\bok\b', r'\bfine\b', r'\bso\s*so\b', r'\bğŸ˜\b', r'\bğŸ˜‘\b'
    ]
}

# Vietnamese abbreviations and slang - Má»Ÿ rá»™ng Ä‘á»ƒ hiá»ƒu teencode
VIETNAMESE_SLANG = {
    # Tá»« viáº¿t táº¯t cÆ¡ báº£n
    'sp': 'sáº£n pháº©m', 'mn': 'má»i ngÆ°á»i', 'ncl': 'nÃ³i chung lÃ ', 're': 'review',
    'com': 'comment', 'men': 'mÃ¬nh', 'de': 'Ä‘á»ƒ', 'd': 'Ä‘Æ°á»£c', 'm': 'mÃ¬nh',
    'n': 'nÃ y', 'k': 'khÃ´ng', 'cx': 'cÅ©ng', 'dc': 'Ä‘Æ°á»£c', 'vs': 'vá»›i',
    'mik': 'mÃ¬nh', 'mk': 'mÃ¬nh', 't': 'tÃ´i', 'e': 'em', 'a': 'anh',
    'c': 'chá»‹', 'b': 'báº¡n', 'shop': 'cá»­a hÃ ng', 'ship': 'giao hÃ ng',
    'shipper': 'ngÆ°á»i giao hÃ ng', 'qte': 'quÃ¡ trá»i', 'qtr': 'quÃ¡ trá»i',
    
    # Teencode phá»• biáº¿n
    'ok': 'ok', 'oki': 'ok', 'oke': 'ok', 'okie': 'ok',
    'nma': 'nhÆ°ng mÃ ', 'nhm': 'nhÆ°ng mÃ ', 'nhma': 'nhÆ°ng mÃ ',
    'gac': 'gÃ¡c', 'gÃ¡c': 'gÃ¡c', 'gak': 'gÃ¡c',
    'dep': 'Ä‘áº¹p', 'dep': 'Ä‘áº¹p', 'xau': 'xáº¥u', 'xau': 'xáº¥u',
    'tot': 'tá»‘t', 'tot': 'tá»‘t', 'te': 'tá»‡', 'te': 'tá»‡',
    'ngon': 'ngon', 'ngon': 'ngon', 'ngon': 'ngon',
    're': 'ráº»', 're': 'ráº»', 'dat': 'Ä‘áº¯t', 'dat': 'Ä‘áº¯t',
    'nhanh': 'nhanh', 'cham': 'cháº­m', 'cham': 'cháº­m',
    'dung': 'Ä‘Ãºng', 'sai': 'sai', 'dung': 'Ä‘Ãºng',
    'hay': 'hay', 'hay': 'hay', 'nhat': 'nháº¥t',
    'nhat': 'nháº¥t', 'cuoi': 'cuá»‘i', 'cuoi': 'cuá»‘i',
    'dau': 'Ä‘áº§u', 'cuoi': 'cuá»‘i', 'giua': 'giá»¯a',
    'tren': 'trÃªn', 'duoi': 'dÆ°á»›i', 'trong': 'trong',
    'ngoai': 'ngoÃ i', 'ben': 'bÃªn', 'canh': 'cáº¡nh',
    
    # Tá»« cáº£m thÃ¡n
    'ui': 'ui', 'oi': 'Ã´i', 'a': 'Ã ', 'e': 'Ã¨',
    'uh': 'á»«', 'um': 'á»«m', 'hmm': 'hmm', 'huh': 'huh',
    'wow': 'wow', 'omg': 'omg', 'wtf': 'wtf',
    
    # Tá»« lÃ³ng máº¡ng
    'lol': 'lol', 'haha': 'haha', 'hehe': 'hehe',
    'kkk': 'kkk', 'hihi': 'hihi', 'hoho': 'hoho',
    'yay': 'yay', 'yeah': 'yeah', 'yep': 'yep',
    'nope': 'nope', 'nah': 'nah', 'meh': 'meh',
    
    # Tá»« ghÃ©p thÆ°á»ng dÃ¹ng
    'rat': 'ráº¥t', 'qua': 'quÃ¡', 'cung': 'cÅ©ng',
    'cung': 'cÅ©ng', 'rat': 'ráº¥t', 'qua': 'quÃ¡',
    'sieu': 'siÃªu', 'cuc': 'cá»±c', 'vo': 'vÃ´',
    'vo': 'vÃ´', 'cung': 'cÅ©ng', 'rat': 'ráº¥t',
    
    # Tá»« viáº¿t táº¯t Ä‘áº·c biá»‡t
    'ko': 'khÃ´ng', 'khong': 'khÃ´ng', 'kg': 'khÃ´ng',
    'k': 'khÃ´ng', 'kh': 'khÃ´ng', 'k': 'khÃ´ng',
    'dc': 'Ä‘Æ°á»£c', 'duoc': 'Ä‘Æ°á»£c', 'd': 'Ä‘Æ°á»£c',
    'd': 'Ä‘Æ°á»£c', 'dc': 'Ä‘Æ°á»£c', 'duoc': 'Ä‘Æ°á»£c',
    'cx': 'cÅ©ng', 'cung': 'cÅ©ng', 'c': 'cÅ©ng',
    'c': 'cÅ©ng', 'cx': 'cÅ©ng', 'cung': 'cÅ©ng',
    
    # Tá»« cáº£m xÃºc
    'vui': 'vui', 'buon': 'buá»“n', 'hanh': 'háº¡nh',
    'hanh': 'háº¡nh', 'phuc': 'phÃºc', 'phuc': 'phÃºc',
    'thich': 'thÃ­ch', 'yeu': 'yÃªu', 'thuong': 'thÆ°Æ¡ng',
    'thuong': 'thÆ°Æ¡ng', 'ghÃ©t': 'ghÃ©t', 'ghet': 'ghÃ©t',
    'chan': 'chÃ¡n', 'nhan': 'nhÃ n', 'nhan': 'nhÃ n',
    
    # Tá»« mÃ´ táº£ sáº£n pháº©m
    'dep': 'Ä‘áº¹p', 'xau': 'xáº¥u', 'tot': 'tá»‘t', 'te': 'tá»‡',
    'ngon': 'ngon', 'dá»Ÿ': 'dá»Ÿ', 'do': 'dá»Ÿ', 'do': 'dá»Ÿ',
    're': 'ráº»', 'dat': 'Ä‘áº¯t', 'nhanh': 'nhanh', 'cham': 'cháº­m',
    'dung': 'Ä‘Ãºng', 'sai': 'sai', 'hay': 'hay', 'nhat': 'nháº¥t',
    'rat': 'ráº¥t', 'qua': 'quÃ¡', 'sieu': 'siÃªu', 'cuc': 'cá»±c',
    'vo': 'vÃ´', 'cung': 'cÅ©ng', 'rat': 'ráº¥t', 'qua': 'quÃ¡'
}

def init_database(db_path: str = DB_PATH) -> None:
    """Khá»Ÿi táº¡o database vá»›i cÃ¡c báº£ng cáº§n thiáº¿t"""
    os.makedirs(Path(db_path).parent, exist_ok=True)
    with sqlite3.connect(db_path) as conn:
        c = conn.cursor()
        
        # XÃ³a báº£ng cÅ© náº¿u cÃ³
        c.execute("DROP TABLE IF EXISTS sentiment_training_data")
        
        # Báº£ng dá»¯ liá»‡u training
        c.execute("""
            CREATE TABLE sentiment_training_data (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                text TEXT NOT NULL,
                original_text TEXT,
                label INTEGER NOT NULL,
                source_file TEXT,
                confidence REAL DEFAULT 1.0,
                preprocessing_info TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # XÃ³a báº£ng cÅ© náº¿u cÃ³
        c.execute("DROP TABLE IF EXISTS model_performance")
        
        # Báº£ng performance model
        c.execute("""
            CREATE TABLE model_performance (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                model_name TEXT NOT NULL,
                accuracy REAL NOT NULL,
                f1_score REAL NOT NULL,
                precision REAL NOT NULL,
                recall REAL NOT NULL,
                training_samples INTEGER NOT NULL,
                validation_samples INTEGER NOT NULL,
                training_time REAL NOT NULL,
                cross_val_scores TEXT,
                confusion_matrix TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # XÃ³a báº£ng cÅ© náº¿u cÃ³
        c.execute("DROP TABLE IF EXISTS sentiment_predictions")
        
        # Báº£ng predictions
        c.execute("""
            CREATE TABLE sentiment_predictions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                text TEXT NOT NULL,
                predicted_label INTEGER NOT NULL,
                confidence REAL NOT NULL,
                probabilities TEXT NOT NULL,
                model_used TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        conn.commit()

def normalize_vietnamese_text(text: str) -> str:
    """Chuáº©n hÃ³a vÄƒn báº£n tiáº¿ng Viá»‡t"""
    if not isinstance(text, str):
        return ""
    
    # Xá»­ lÃ½ tá»« ghÃ©p cÃ³ dáº¥u gáº¡ch dÆ°á»›i trÆ°á»›c
    text = expand_underscore_words(text)
    
    # Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t khÃ´ng cáº§n thiáº¿t nhÆ°ng giá»¯ láº¡i dáº¥u cÃ¢u quan trá»ng
    text = re.sub(r'[^\w\s.,!?;:()\-/]', ' ', text)
    
    # Chuáº©n hÃ³a khoáº£ng tráº¯ng
    text = re.sub(r'\s+', ' ', text)
    
    # Loáº¡i bá» dáº¥u cÃ¢u thá»«a
    text = re.sub(r'[.,!?;:]{2,}', lambda m: m.group(0)[0], text)
    
    return text.strip()

def expand_underscore_words(text: str) -> str:
    """Xá»­ lÃ½ tá»« ghÃ©p cÃ³ dáº¥u gáº¡ch dÆ°á»›i"""
    # Dictionary cÃ¡c tá»« ghÃ©p phá»• biáº¿n vá»›i dáº¥u gáº¡ch dÆ°á»›i
    underscore_words = {
        'sáº£n_pháº©m': 'sáº£n pháº©m',
        'cháº¥t_lÆ°á»£ng': 'cháº¥t lÆ°á»£ng', 
        'thá»i_gian': 'thá»i gian',
        'mÃ u_sáº¯c': 'mÃ u sáº¯c',
        'kÃ­ch_thÆ°á»›c': 'kÃ­ch thÆ°á»›c',
        'giÃ¡_cáº£': 'giÃ¡ cáº£',
        'giÃ¡_thÃ nh': 'giÃ¡ thÃ nh',
        'hÃ¬nh_áº£nh': 'hÃ¬nh áº£nh',
        'mÃ´_táº£': 'mÃ´ táº£',
        'chi_tiáº¿t': 'chi tiáº¿t',
        'thÃ´ng_tin': 'thÃ´ng tin',
        'dá»‹ch_vá»¥': 'dá»‹ch vá»¥',
        'khÃ¡ch_hÃ ng': 'khÃ¡ch hÃ ng',
        'cá»­a_hÃ ng': 'cá»­a hÃ ng',
        'giao_hÃ ng': 'giao hÃ ng',
        'thanh_toÃ¡n': 'thanh toÃ¡n',
        'Ä‘Ã¡nh_giÃ¡': 'Ä‘Ã¡nh giÃ¡',
        'pháº£n_há»“i': 'pháº£n há»“i',
        'há»—_trá»£': 'há»— trá»£',
        'tÆ°_váº¥n': 'tÆ° váº¥n',
        'báº£o_hiá»ƒm': 'báº£o hiá»ƒm',
        'báº£o_quáº£n': 'báº£o quáº£n',
        'váº­n_chuyá»ƒn': 'váº­n chuyá»ƒn',
        'phÃ­_ship': 'phÃ­ ship',
        'miá»…n_phÃ­': 'miá»…n phÃ­',
        'giáº£m_giÃ¡': 'giáº£m giÃ¡',
        'khuyáº¿n_mÃ£i': 'khuyáº¿n mÃ£i',
        'Æ°u_Ä‘Ã£i': 'Æ°u Ä‘Ã£i',
        'chÆ°Æ¡ng_trÃ¬nh': 'chÆ°Æ¡ng trÃ¬nh',
        'sá»±_kiá»‡n': 'sá»± kiá»‡n',
        'tin_tá»©c': 'tin tá»©c',
        'bÃ i_viáº¿t': 'bÃ i viáº¿t',
        'ná»™i_dung': 'ná»™i dung',
        'chá»§_Ä‘á»': 'chá»§ Ä‘á»',
        'danh_má»¥c': 'danh má»¥c',
        'loáº¡i_sáº£n_pháº©m': 'loáº¡i sáº£n pháº©m',
        'thÆ°Æ¡ng_hiá»‡u': 'thÆ°Æ¡ng hiá»‡u',
        'nhÃ£n_hiá»‡u': 'nhÃ£n hiá»‡u',
        'xuáº¥t_xá»©': 'xuáº¥t xá»©',
        'nÆ¡i_sáº£n_xuáº¥t': 'nÆ¡i sáº£n xuáº¥t',
        'háº¡n_sá»­_dá»¥ng': 'háº¡n sá»­ dá»¥ng',
        'ngÃ y_sáº£n_xuáº¥t': 'ngÃ y sáº£n xuáº¥t',
        'trá»ng_lÆ°á»£ng': 'trá»ng lÆ°á»£ng',
        'kÃ­ch_thÆ°á»›c': 'kÃ­ch thÆ°á»›c',
        'chiá»u_dÃ i': 'chiá»u dÃ i',
        'chiá»u_rá»™ng': 'chiá»u rá»™ng',
        'chiá»u_cao': 'chiá»u cao',
        'Ä‘Æ°á»ng_kÃ­nh': 'Ä‘Æ°á»ng kÃ­nh',
        'chu_vi': 'chu vi',
        'diá»‡n_tÃ­ch': 'diá»‡n tÃ­ch',
        'thá»ƒ_tÃ­ch': 'thá»ƒ tÃ­ch',
        'dung_lÆ°á»£ng': 'dung lÆ°á»£ng',
        'cÃ´ng_suáº¥t': 'cÃ´ng suáº¥t',
        'hiá»‡u_suáº¥t': 'hiá»‡u suáº¥t',
        'cháº¥t_lÆ°á»£ng': 'cháº¥t lÆ°á»£ng',
        'Ä‘á»™_bá»n': 'Ä‘á»™ bá»n',
        'tuá»•i_thá»': 'tuá»•i thá»',
        'thá»i_gian': 'thá»i gian',
        'tá»‘c_Ä‘á»™': 'tá»‘c Ä‘á»™',
        'nhiá»‡t_Ä‘á»™': 'nhiá»‡t Ä‘á»™',
        'Ã¡p_suáº¥t': 'Ã¡p suáº¥t',
        'Ä‘á»™_áº©m': 'Ä‘á»™ áº©m',
        'Ã¡nh_sÃ¡ng': 'Ã¡nh sÃ¡ng',
        'Ã¢m_thanh': 'Ã¢m thanh',
        'mÃ¹i_hÆ°Æ¡ng': 'mÃ¹i hÆ°Æ¡ng',
        'vá»‹_giÃ¡c': 'vá»‹ giÃ¡c',
        'xÃºc_giÃ¡c': 'xÃºc giÃ¡c',
        'thá»‹_giÃ¡c': 'thá»‹ giÃ¡c',
        'thÃ­nh_giÃ¡c': 'thÃ­nh giÃ¡c',
        'khá»©u_giÃ¡c': 'khá»©u giÃ¡c',
        'cáº£m_giÃ¡c': 'cáº£m giÃ¡c',
        'cáº£m_xÃºc': 'cáº£m xÃºc',
        'tÃ¢m_tráº¡ng': 'tÃ¢m tráº¡ng',
        'tinh_tháº§n': 'tinh tháº§n',
        'thá»ƒ_cháº¥t': 'thá»ƒ cháº¥t',
        'sá»©c_khá»e': 'sá»©c khá»e',
        'an_toÃ n': 'an toÃ n',
        'tiá»‡n_lá»£i': 'tiá»‡n lá»£i',
        'dá»…_dÃ¹ng': 'dá»… dÃ¹ng',
        'khÃ³_dÃ¹ng': 'khÃ³ dÃ¹ng',
        'thuáº­n_tiá»‡n': 'thuáº­n tiá»‡n',
        'báº¥t_tiá»‡n': 'báº¥t tiá»‡n',
        'hÃ i_lÃ²ng': 'hÃ i lÃ²ng',
        'tháº¥t_vá»ng': 'tháº¥t vá»ng',
        'Æ°ng_Ã½': 'Æ°ng Ã½',
        'khÃ´ng_Æ°ng_Ã½': 'khÃ´ng Æ°ng Ã½',
        'thÃ­ch': 'thÃ­ch',
        'khÃ´ng_thÃ­ch': 'khÃ´ng thÃ­ch',
        'yÃªu': 'yÃªu',
        'ghÃ©t': 'ghÃ©t',
        'quan_tÃ¢m': 'quan tÃ¢m',
        'khÃ´ng_quan_tÃ¢m': 'khÃ´ng quan tÃ¢m',
        'chÃº_Ã½': 'chÃº Ã½',
        'bá»_qua': 'bá» qua',
        'quan_trá»ng': 'quan trá»ng',
        'khÃ´ng_quan_trá»ng': 'khÃ´ng quan trá»ng',
        'cáº§n_thiáº¿t': 'cáº§n thiáº¿t',
        'khÃ´ng_cáº§n_thiáº¿t': 'khÃ´ng cáº§n thiáº¿t',
        'há»¯u_Ã­ch': 'há»¯u Ã­ch',
        'vÃ´_Ã­ch': 'vÃ´ Ã­ch',
        'cÃ³_Ã­ch': 'cÃ³ Ã­ch',
        'khÃ´ng_cÃ³_Ã­ch': 'khÃ´ng cÃ³ Ã­ch',
        'hiá»‡u_quáº£': 'hiá»‡u quáº£',
        'khÃ´ng_hiá»‡u_quáº£': 'khÃ´ng hiá»‡u quáº£',
        'thÃ nh_cÃ´ng': 'thÃ nh cÃ´ng',
        'tháº¥t_báº¡i': 'tháº¥t báº¡i',
        'tá»‘t': 'tá»‘t',
        'xáº¥u': 'xáº¥u',
        'Ä‘áº¹p': 'Ä‘áº¹p',
        'xáº¥u': 'xáº¥u',
        'hay': 'hay',
        'dá»Ÿ': 'dá»Ÿ',
        'ngon': 'ngon',
        'dá»Ÿ': 'dá»Ÿ',
        'ráº»': 'ráº»',
        'Ä‘áº¯t': 'Ä‘áº¯t',
        'nhanh': 'nhanh',
        'cháº­m': 'cháº­m',
        'Ä‘Ãºng': 'Ä‘Ãºng',
        'sai': 'sai',
        'chÃ­nh_xÃ¡c': 'chÃ­nh xÃ¡c',
        'khÃ´ng_chÃ­nh_xÃ¡c': 'khÃ´ng chÃ­nh xÃ¡c',
        'rÃµ_rÃ ng': 'rÃµ rÃ ng',
        'khÃ´ng_rÃµ_rÃ ng': 'khÃ´ng rÃµ rÃ ng',
        'dá»…_hiá»ƒu': 'dá»… hiá»ƒu',
        'khÃ³_hiá»ƒu': 'khÃ³ hiá»ƒu',
        'Ä‘Æ¡n_giáº£n': 'Ä‘Æ¡n giáº£n',
        'phá»©c_táº¡p': 'phá»©c táº¡p',
        'dá»…_dÃ ng': 'dá»… dÃ ng',
        'khÃ³_khÄƒn': 'khÃ³ khÄƒn',
        'thuáº­n_lá»£i': 'thuáº­n lá»£i',
        'báº¥t_lá»£i': 'báº¥t lá»£i',
        'cÃ³_lá»£i': 'cÃ³ lá»£i',
        'khÃ´ng_cÃ³_lá»£i': 'khÃ´ng cÃ³ lá»£i',
        'cÃ³_Ã­ch': 'cÃ³ Ã­ch',
        'khÃ´ng_cÃ³_Ã­ch': 'khÃ´ng cÃ³ Ã­ch',
        'há»¯u_dá»¥ng': 'há»¯u dá»¥ng',
        'vÃ´_dá»¥ng': 'vÃ´ dá»¥ng',
        'cáº§n_thiáº¿t': 'cáº§n thiáº¿t',
        'khÃ´ng_cáº§n_thiáº¿t': 'khÃ´ng cáº§n thiáº¿t',
        'quan_trá»ng': 'quan trá»ng',
        'khÃ´ng_quan_trá»ng': 'khÃ´ng quan trá»ng',
        'cáº§n_thiáº¿t': 'cáº§n thiáº¿t',
        'khÃ´ng_cáº§n_thiáº¿t': 'khÃ´ng cáº§n thiáº¿t'
    }
    
    # Thay tháº¿ tá»« ghÃ©p cÃ³ dáº¥u gáº¡ch dÆ°á»›i
    for underscore_word, normal_word in underscore_words.items():
        text = text.replace(underscore_word, normal_word)
    
    return text

def expand_vietnamese_slang(text: str) -> str:
    """Má»Ÿ rá»™ng tá»« viáº¿t táº¯t tiáº¿ng Viá»‡t vÃ  teencode"""
    words = text.split()
    expanded = []
    
    for word in words:
        # Loáº¡i bá» dáº¥u cÃ¢u táº¡m thá»i
        clean_word = re.sub(r'[^\w]', '', word.lower())
        punctuation = re.sub(r'[\w]', '', word)
        
        # Kiá»ƒm tra tá»« viáº¿t táº¯t
        if clean_word in VIETNAMESE_SLANG:
            expanded_word = VIETNAMESE_SLANG[clean_word] + punctuation
            expanded.append(expanded_word)
        else:
            # Kiá»ƒm tra teencode phá»©c táº¡p hÆ¡n
            expanded_word = expand_teencode(clean_word) + punctuation
            expanded.append(expanded_word)
    
    return ' '.join(expanded)

def expand_teencode(word: str) -> str:
    """Xá»­ lÃ½ teencode phá»©c táº¡p"""
    # Giá»¯ nguyÃªn náº¿u Ä‘Ã£ lÃ  tá»« chuáº©n
    if word in VIETNAMESE_SLANG:
        return VIETNAMESE_SLANG[word]
    
    # Xá»­ lÃ½ cÃ¡c pattern teencode phá»• biáº¿n
    patterns = {
        # Thay tháº¿ kÃ½ tá»±
        r'^k$': 'khÃ´ng',
        r'^ko$': 'khÃ´ng', 
        r'^kg$': 'khÃ´ng',
        r'^kh$': 'khÃ´ng',
        r'^dc$': 'Ä‘Æ°á»£c',
        r'^d$': 'Ä‘Æ°á»£c',
        r'^cx$': 'cÅ©ng',
        r'^c$': 'cÅ©ng',
        r'^vs$': 'vá»›i',
        r'^n$': 'nÃ y',
        r'^m$': 'mÃ¬nh',
        r'^t$': 'tÃ´i',
        r'^e$': 'em',
        r'^a$': 'anh',
        r'^c$': 'chá»‹',
        r'^b$': 'báº¡n',
        
        # Tá»« cáº£m thÃ¡n
        r'^ok$': 'ok',
        r'^oki$': 'ok',
        r'^oke$': 'ok',
        r'^okie$': 'ok',
        r'^wow$': 'wow',
        r'^omg$': 'omg',
        r'^lol$': 'lol',
        r'^haha$': 'haha',
        r'^hehe$': 'hehe',
        r'^yay$': 'yay',
        r'^yeah$': 'yeah',
        r'^yep$': 'yep',
        r'^nope$': 'nope',
        r'^nah$': 'nah',
        r'^meh$': 'meh',
        
        # Tá»« mÃ´ táº£
        r'^dep$': 'Ä‘áº¹p',
        r'^xau$': 'xáº¥u',
        r'^tot$': 'tá»‘t',
        r'^te$': 'tá»‡',
        r'^ngon$': 'ngon',
        r'^re$': 'ráº»',
        r'^dat$': 'Ä‘áº¯t',
        r'^nhanh$': 'nhanh',
        r'^cham$': 'cháº­m',
        r'^dung$': 'Ä‘Ãºng',
        r'^sai$': 'sai',
        r'^hay$': 'hay',
        r'^nhat$': 'nháº¥t',
        r'^rat$': 'ráº¥t',
        r'^qua$': 'quÃ¡',
        r'^sieu$': 'siÃªu',
        r'^cuc$': 'cá»±c',
        r'^vo$': 'vÃ´',
        r'^cung$': 'cÅ©ng',
        
        # Tá»« ghÃ©p
        r'^nma$': 'nhÆ°ng mÃ ',
        r'^nhm$': 'nhÆ°ng mÃ ',
        r'^nhma$': 'nhÆ°ng mÃ ',
        r'^gac$': 'gÃ¡c',
        r'^gak$': 'gÃ¡c',
        r'^qte$': 'quÃ¡ trá»i',
        r'^qtr$': 'quÃ¡ trá»i',
        r'^sp$': 'sáº£n pháº©m',
        r'^mn$': 'má»i ngÆ°á»i',
        r'^ncl$': 'nÃ³i chung lÃ ',
        r'^re$': 'review',
        r'^com$': 'comment',
        r'^men$': 'mÃ¬nh',
        r'^de$': 'Ä‘á»ƒ',
        r'^mik$': 'mÃ¬nh',
        r'^mk$': 'mÃ¬nh',
        r'^shop$': 'cá»­a hÃ ng',
        r'^ship$': 'giao hÃ ng',
        r'^shipper$': 'ngÆ°á»i giao hÃ ng',
    }
    
    for pattern, replacement in patterns.items():
        if re.match(pattern, word):
            return replacement
    
    # Giá»¯ nguyÃªn náº¿u khÃ´ng tÃ¬m tháº¥y pattern
    return word

def extract_sentiment_features(text: str) -> Dict[str, Any]:
    """TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng sentiment tá»« vÄƒn báº£n"""
    text_lower = text.lower()
    features = {
        'positive_count': 0,
        'negative_count': 0,
        'neutral_count': 0,
        'exclamation_count': text.count('!'),
        'question_count': text.count('?'),
        'caps_ratio': sum(1 for c in text if c.isupper()) / max(len(text), 1),
        'length': len(text),
        'word_count': len(text.split())
    }
    
    # Äáº¿m pattern sentiment
    for pattern in VIETNAMESE_PATTERNS['positive']:
        features['positive_count'] += len(re.findall(pattern, text_lower))
    
    for pattern in VIETNAMESE_PATTERNS['negative']:
        features['negative_count'] += len(re.findall(pattern, text_lower))
    
    for pattern in VIETNAMESE_PATTERNS['neutral']:
        features['neutral_count'] += len(re.findall(pattern, text_lower))
    
    return features

def preprocess_text(text: str) -> Tuple[str, Dict[str, Any]]:
    """Xá»­ lÃ½ vÄƒn báº£n tiáº¿ng Viá»‡t toÃ n diá»‡n"""
    original_text = text
    
    # Chuáº©n hÃ³a cÆ¡ báº£n
    text = normalize_vietnamese_text(text)
    
    # Má»Ÿ rá»™ng tá»« viáº¿t táº¯t
    text = expand_vietnamese_slang(text)
    
    # TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng
    features = extract_sentiment_features(text)
    
    return text, features

def load_shopee_data(data_dir: str) -> pd.DataFrame:
    """Táº£i vÃ  xá»­ lÃ½ dá»¯ liá»‡u Shopee"""
    data_root = Path(data_dir)
    if not data_root.exists():
        raise FileNotFoundError(f"Data directory not found: {data_root}")

    parts = []
    
    # Load train.csv vÃ  val.csv
    for name in ["train.csv", "val.csv"]:
        f = data_root / name
        if f.exists():
            try:
                df = pd.read_csv(f, encoding='utf-8')
                df["source"] = name
                parts.append(df)
                print(f"âœ… Loaded {name}: {len(df)} rows")
            except Exception as e:
                print(f"âš ï¸ Error loading {name}: {e}")
    
    # Load automated/*.csv
    auto_dir = data_root / "automated"
    if auto_dir.exists():
        for f in auto_dir.glob("*.csv"):
            try:
                # Thá»­ Ä‘á»c vá»›i cÃ¡c options khÃ¡c nhau cho file cÃ³ váº¥n Ä‘á»
                try:
                    df = pd.read_csv(f, encoding='utf-8')
                except pd.errors.ParserError:
                    # Náº¿u lá»—i parsing, thá»­ vá»›i options khÃ¡c
                    df = pd.read_csv(f, encoding='utf-8', sep=None, engine='python', on_bad_lines='skip')
                except Exception:
                    # Cuá»‘i cÃ¹ng thá»­ vá»›i delimiter tá»± Ä‘á»™ng
                    df = pd.read_csv(f, encoding='utf-8', sep=None, engine='python', 
                                   quoting=0, on_bad_lines='skip', error_bad_lines=False)
                
                df["source"] = f.name
                parts.append(df)
                print(f"âœ… Loaded {f.name}: {len(df)} rows")
            except Exception as e:
                print(f"âš ï¸ Error loading {f.name}: {e}")
                # Thá»­ Ä‘á»c vá»›i pandas engine python vÃ  bá» qua dÃ²ng lá»—i
                try:
                    df = pd.read_csv(f, encoding='utf-8', engine='python', 
                                   on_bad_lines='skip', error_bad_lines=False)
                    df["source"] = f.name
                    parts.append(df)
                    print(f"âœ… Loaded {f.name} (with errors skipped): {len(df)} rows")
                except Exception as e2:
                    print(f"âŒ Failed to load {f.name}: {e2}")

    if not parts:
        raise ValueError("No CSV files found in the provided directory")

    df_all = pd.concat(parts, ignore_index=True)
    print(f"ğŸ“Š Total loaded: {len(df_all)} rows")

    # Map text column
    text_col = None
    for col in ["text", "review", "txt"]:
        if col in df_all.columns:
            text_col = col
            break
    
    if text_col is None:
        raise ValueError("No text column found (text/review/txt)")
    
    df_all["original_text"] = df_all[text_col].astype(str)
    
    # Map label column
    label_col = None
    for col in ["label", "sentiment", "lbl"]:
        if col in df_all.columns:
            label_col = col
            break
    
    if label_col is None:
        raise ValueError("No label column found (label/sentiment/lbl)")
    
    df_all["sentiment"] = df_all[label_col]

    # Preprocess text
    print("ğŸ”„ Preprocessing texts...")
    processed_data = []
    for idx, row in df_all.iterrows():
        if idx % 1000 == 0:
            print(f"  Processed {idx}/{len(df_all)}")
        
        processed_text, features = preprocess_text(row["original_text"])
        processed_data.append({
            'text': processed_text,
            'original_text': row["original_text"],
            'sentiment': row["sentiment"],
            'source': row["source"],
            'features': features
        })
    
    df_processed = pd.DataFrame(processed_data)
    
    # Map sentiment to 3-class
    def map_sentiment(val):
        if isinstance(val, str):
            v = val.lower().strip()
            if v in {"neg", "negative", "0"}: return "negative"
            if v in {"pos", "positive", "2"}: return "positive"
            if v in {"neu", "neutral", "1"}: return "neutral"
            return "neutral"  # default
        try:
            iv = int(val)
            if iv == 0: return "negative"   # 0 = negative
            if iv == 1: return "neutral"    # 1 = neutral
            if iv == 2: return "positive"   # 2 = positive
        except:
            pass
        return "neutral"
    
    df_processed["sentiment"] = df_processed["sentiment"].apply(map_sentiment)
    
    # Táº¡o neutral tá»« uncertainty náº¿u cÃ³
    if "prob_positve" in df_all.columns:
        uncertainty_mask = (df_all["prob_positve"] >= 0.4) & (df_all["prob_positve"] <= 0.6)
        uncertainty_data = []
        for idx, row in df_all[uncertainty_mask].iterrows():
            # Kiá»ƒm tra dá»¯ liá»‡u trÆ°á»›c khi xá»­ lÃ½
            if pd.isna(row[text_col]) or str(row[text_col]).strip() == "" or str(row[text_col]).lower() == "nan":
                continue
                
            processed_text, features = preprocess_text(row[text_col])
            
            # Kiá»ƒm tra káº¿t quáº£ xá»­ lÃ½
            if processed_text == "nan" or processed_text.strip() == "":
                continue
                
            uncertainty_data.append({
                'text': processed_text,
                'original_text': row[text_col],
                'sentiment': 'neutral',
                'source': f"{row.get('source', 'unknown')}_uncertainty",
                'features': features
            })
        
        if uncertainty_data:
            df_uncertainty = pd.DataFrame(uncertainty_data)
            df_processed = pd.concat([df_processed, df_uncertainty], ignore_index=True)
            print(f"âœ… Added {len(df_uncertainty)} uncertainty samples as neutral")
        else:
            print("âš ï¸ No valid uncertainty samples found")

    # Clean data
    print(f"ğŸ“Š Before cleaning: {len(df_processed)} rows")
    df_processed = df_processed.dropna(subset=["text", "sentiment"])
    print(f"ğŸ“Š After dropna: {len(df_processed)} rows")
    
    # Loáº¡i bá» dá»¯ liá»‡u lá»—i - text = "nan" hoáº·c chá»‰ cÃ³ kÃ½ tá»± Ä‘áº·c biá»‡t
    df_processed = df_processed[df_processed["text"] != "nan"]
    df_processed = df_processed[df_processed["text"] != "NaN"]
    df_processed = df_processed[df_processed["text"] != ""]
    df_processed = df_processed[df_processed["text"].str.strip() != ""]
    print(f"ğŸ“Š After removing 'nan' and empty: {len(df_processed)} rows")
    
    # Kiá»ƒm tra Ä‘á»™ dÃ i text trÆ°á»›c khi filter
    text_lengths = df_processed["text"].str.len()
    print(f"ğŸ“Š Text length stats: min={text_lengths.min()}, max={text_lengths.max()}, mean={text_lengths.mean():.2f}")
    print(f"ğŸ“Š Text length distribution:")
    print(text_lengths.value_counts().head(10))
    
    # Filter nháº¹ hÆ¡n - chá»‰ loáº¡i bá» text quÃ¡ ngáº¯n hoáº·c chá»‰ cÃ³ kÃ½ tá»± Ä‘áº·c biá»‡t
    df_processed = df_processed[df_processed["text"].str.len() > 1]  # Giáº£m tá»« 3 xuá»‘ng 1
    df_processed = df_processed[df_processed["text"].str.strip().str.len() > 0]  # Loáº¡i bá» text chá»‰ cÃ³ space
    print(f"ğŸ“Š After length filter: {len(df_processed)} rows")
    
    print(f"ğŸ“Š Final dataset: {len(df_processed)} rows")
    print(f"ğŸ“Š Sentiment distribution:")
    print(df_processed["sentiment"].value_counts())
    
    return df_processed[["text", "original_text", "sentiment", "source", "features"]]

def save_training_data(df: pd.DataFrame, db_path: str = DB_PATH) -> None:
    """LÆ°u dá»¯ liá»‡u training vÃ o SQLite"""
    with sqlite3.connect(db_path) as conn:
        c = conn.cursor()
        # Chá»‰ xÃ³a dá»¯ liá»‡u cÅ© náº¿u Ä‘Ã¢y lÃ  láº§n Ä‘áº§u tiÃªn
        c.execute("SELECT COUNT(*) FROM sentiment_training_data")
        if c.fetchone()[0] > 0:
            print("âš ï¸ Database already has data. Appending new data...")
        else:
            print("ğŸ“ First time saving data to database...")
        
        rows = []
        for _, row in df.iterrows():
            features_json = json.dumps(row["features"]) if "features" in row else "{}"
            rows.append((
                row["text"],
                row["original_text"],
                LABEL_MAP[row["sentiment"]],
                row["source"],
                1.0,
                features_json
            ))
        
        c.executemany("""
            INSERT INTO sentiment_training_data 
            (text, original_text, label, source_file, confidence, preprocessing_info)
            VALUES (?, ?, ?, ?, ?, ?)
        """, rows)
        conn.commit()
        print(f"âœ… Saved {len(rows)} training samples to SQLite")

def build_advanced_model() -> Pipeline:
    """XÃ¢y dá»±ng model nÃ¢ng cao cho tiáº¿ng Viá»‡t"""
    return Pipeline([
        ("tfidf", TfidfVectorizer(
            max_features=100000,
            ngram_range=(1, 3),  # unigram, bigram, trigram
            lowercase=True,
            strip_accents="unicode",
            min_df=2,
            max_df=0.95,
            sublinear_tf=True,  # log scaling
        )),
        ("clf", LogisticRegression(
            random_state=42, 
            max_iter=1000,
            multi_class='ovr',  # One-vs-Rest for multi-class
            class_weight='balanced'  # Handle class imbalance
        ))
    ])

def train_and_evaluate(df: pd.DataFrame, db_path: str = DB_PATH) -> Dict[str, Any]:
    """Training vÃ  Ä‘Ã¡nh giÃ¡ model"""
    print("ğŸ”„ Preparing training data...")
    
    # CÃ¢n báº±ng dá»¯ liá»‡u
    counts = df["sentiment"].value_counts()
    print(f"ğŸ“Š Original distribution: {counts.to_dict()}")
    
    min_count = counts.min()
    balanced_dfs = []
    for sentiment in counts.index:
        sentiment_df = df[df["sentiment"] == sentiment]
        if len(sentiment_df) > min_count:
            balanced_df = sentiment_df.sample(min_count, random_state=42)
        else:
            balanced_df = sentiment_df
        balanced_dfs.append(balanced_df)
    
    balanced_df = pd.concat(balanced_dfs, ignore_index=True)
    print(f"ğŸ“Š Balanced distribution: {balanced_df['sentiment'].value_counts().to_dict()}")
    
    # Prepare features
    X = balanced_df["text"].tolist()
    y = [LABEL_MAP[s] for s in balanced_df["sentiment"].tolist()]
    
    # Split data
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"ğŸ“Š Training samples: {len(X_train)}")
    print(f"ğŸ“Š Validation samples: {len(X_val)}")
    
    # Build and train model
    print("ğŸ”„ Training model...")
    model = build_advanced_model()
    model.fit(X_train, y_train)
    
    # Evaluate
    print("ğŸ”„ Evaluating model...")
    y_pred = model.predict(X_val)
    accuracy = accuracy_score(y_val, y_pred)
    
    # Cross-validation
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
    
    # Classification report
    report = classification_report(y_val, y_pred, output_dict=True, zero_division=0)
    
    # Confusion matrix
    cm = confusion_matrix(y_val, y_pred)
    
    # Save model
    os.makedirs(MODEL_DIR, exist_ok=True)
    model_data = {
        "pipeline": model,
        "label_map": LABEL_MAP,
        "id2label": ID2LABEL,
        "preprocessing_info": {
            "vietnamese_patterns": VIETNAMESE_PATTERNS,
            "slang_dict": VIETNAMESE_SLANG
        }
    }
    joblib.dump(model_data, MODEL_PATH)
    
    # Save performance to database
    with sqlite3.connect(db_path) as conn:
        c = conn.cursor()
        c.execute("""
            INSERT INTO model_performance
            (model_name, accuracy, f1_score, precision, recall, training_samples, 
             validation_samples, training_time, cross_val_scores, confusion_matrix)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            "advanced-vietnamese-sentiment",
            float(accuracy),
            float(report["macro avg"]["f1-score"]),
            float(report["macro avg"]["precision"]),
            float(report["macro avg"]["recall"]),
            len(X_train),
            len(X_val),
            0.0,  # training time not measured
            json.dumps(cv_scores.tolist()),
            json.dumps(cm.tolist())
        ))
        conn.commit()
    
    print(f"âœ… Model trained successfully!")
    print(f"ğŸ“Š Accuracy: {accuracy:.4f}")
    print(f"ğŸ“Š Cross-validation scores: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
    print(f"ğŸ’¾ Model saved to: {MODEL_PATH}")
    
    return {
        "accuracy": accuracy,
        "cv_scores": cv_scores,
        "report": report,
        "confusion_matrix": cm,
        "model_path": MODEL_PATH
    }

def test_model_predictions(model_path: str, db_path: str = DB_PATH) -> None:
    """Test model vá»›i cÃ¡c cÃ¢u máº«u"""
    print("ğŸ”® Testing model predictions...")
    
    model_data = joblib.load(model_path)
    model = model_data["pipeline"]
    
    test_samples = [
        # CÃ¢u chuáº©n
        "Sáº£n pháº©m ráº¥t tá»‘t, cháº¥t lÆ°á»£ng cao, giao hÃ ng nhanh",
        "TÃ´i khÃ´ng hÃ i lÃ²ng vá»›i dá»‹ch vá»¥ nÃ y, tá»‡ quÃ¡",
        "Sáº£n pháº©m bÃ¬nh thÆ°á»ng, khÃ´ng cÃ³ gÃ¬ Ä‘áº·c biá»‡t",
        "Shop phá»¥c vá»¥ tá»‘t, shipper thÃ¢n thiá»‡n",
        "HÃ ng vá» bá»‹ há»ng, ráº¥t tháº¥t vá»ng",
        "QuÃ¡ tuyá»‡t vá»i! Sáº½ mua láº¡i",
        "KhÃ´ng nhÆ° mong Ä‘á»£i, cháº¥t lÆ°á»£ng kÃ©m",
        "BÃ¬nh thÆ°á»ng thÃ´i, khÃ´ng cÃ³ gÃ¬ ná»•i báº­t",
        "Tuyá»‡t vá»i! Ráº¥t hÃ i lÃ²ng vá»›i sáº£n pháº©m",
        
        # Teencode vÃ  tá»« viáº¿t táº¯t
        "Sp ok, táº¡m dc",
        "Nma sp rat tot, giao hang nhanh",
        "K dc, sp te qua",
        "Oki, sp dep nma hoi dat",
        "Sp rat hay, se mua lai",
        "K nhu mong doi, chat luong kem",
        "Sp binh thuong, k co gi dac biet",
        "Shop phuc vu tot, shipper than thien",
        "Hang ve bi hong, rat that vong",
        "Qua tuyet voi! Se mua lai",
        
        # Teencode phá»©c táº¡p
        "Sp rat tot, chat luong cao, giao hang nhanh",
        "T k hai long vs dich vu nay, te qua",
        "Sp binh thuong, k co gi dac biet",
        "Shop phuc vu tot, shipper than thien",
        "Hang ve bi hong, rat that vong",
        "Sp ok, tam dc",
        "Qua tuyet voi! Se mua lai",
        "K nhu mong doi, chat luong kem",
        "Binh thuong thoi, k co gi noi bat",
        "Tuyet voi! Rat hai long vs sp",
        
        # Tá»« ghÃ©p cÃ³ dáº¥u gáº¡ch dÆ°á»›i
        "Sáº£n_pháº©m ráº¥t tá»‘t, cháº¥t_lÆ°á»£ng cao",
        "Dá»‹ch_vá»¥ giao_hÃ ng nhanh chÃ³ng",
        "KhÃ¡ch_hÃ ng hÃ i_lÃ²ng vá»›i sáº£n_pháº©m",
        "Cá»­a_hÃ ng phá»¥c_vá»¥ tá»‘t, shipper thÃ¢n_thiá»‡n",
        "HÃ ng vá» bá»‹ há»ng, ráº¥t tháº¥t_vá»ng",
        "Sáº£n_pháº©m bÃ¬nh_thÆ°á»ng, khÃ´ng cÃ³ gÃ¬ Ä‘áº·c_biá»‡t",
        "Cháº¥t_lÆ°á»£ng sáº£n_pháº©m kÃ©m, giÃ¡_cáº£ Ä‘áº¯t",
        "Thá»i_gian giao_hÃ ng cháº­m, dá»‹ch_vá»¥ tá»‡",
        "Sáº£n_pháº©m Ä‘áº¹p, mÃ u_sáº¯c Ä‘Ãºng vá»›i mÃ´_táº£",
        "KÃ­ch_thÆ°á»›c sáº£n_pháº©m phÃ¹_há»£p, giÃ¡_thÃ nh há»£p_lÃ½"
    ]
    
    with sqlite3.connect(db_path) as conn:
        c = conn.cursor()
        
        for text in test_samples:
            # Preprocess
            processed_text, _ = preprocess_text(text)
            
            # Predict
            pred_proba = model.predict_proba([processed_text])[0]
            pred_label = np.argmax(pred_proba)
            confidence = pred_proba[pred_label]
            
            # Map to 3-class if only 2 classes
            if len(pred_proba) == 2:
                # Binary case: map to 3-class
                if pred_label == 0:  # negative
                    pred_label = 0
                else:  # positive
                    pred_label = 2
                # Add neutral probability
                neutral_prob = 0.1
                pred_proba = np.array([pred_proba[0], neutral_prob, pred_proba[1]])
                pred_proba = pred_proba / pred_proba.sum()  # normalize
            
            # Save prediction
            c.execute("""
                INSERT INTO sentiment_predictions 
                (text, predicted_label, confidence, probabilities, model_used)
                VALUES (?, ?, ?, ?, ?)
            """, (
                text,
                int(pred_label),
                float(confidence),
                json.dumps({ID2LABEL[i]: float(pred_proba[i]) for i in range(len(pred_proba))}),
                "advanced-vietnamese-sentiment"
            ))
            
            print(f"ğŸ“ '{text[:50]}...' â†’ {ID2LABEL[pred_label]} ({confidence:.3f})")
        
        conn.commit()
    
    print("âœ… Sample predictions saved to database")

def main():
    print("ğŸš€ Advanced Vietnamese Sentiment Training")
    print("=" * 60)
    
    data_dir = sys.argv[1] if len(sys.argv) > 1 else "../shopee-reviews-sentiment-analysis/data"
    print(f"ğŸ“ Data directory: {data_dir}")
    
    # Initialize database
    init_database(DB_PATH)
    
    # Load and preprocess data
    df = load_shopee_data(data_dir)
    
    # Save training data
    save_training_data(df, DB_PATH)
    
    # Train model
    results = train_and_evaluate(df, DB_PATH)
    
    # Test predictions
    test_model_predictions(results["model_path"], DB_PATH)
    
    print("\nğŸ‰ Training completed successfully!")
    print(f"ğŸ“Š Final accuracy: {results['accuracy']:.4f}")
    print(f"ğŸ’¾ Model saved to: {results['model_path']}")
    print(f"ğŸ’¾ Database: {DB_PATH}")

if __name__ == "__main__":
    main()
